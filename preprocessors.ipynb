{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class IswPreprocessor:\n",
    "    def __init__(self, filename):\n",
    "        print(' ------ Preprocssing ISW German corpus ------')\n",
    "        self.file = self.load_isw_tsv_file(filename)\n",
    "        self.ners_vals=[]\n",
    "\n",
    "    def load_isw_tsv_file(self, filename='data/test-full-isw-release.tsv'):\n",
    "        file = open(filename, encoding='utf-8')\n",
    "        return file\n",
    "\n",
    "    def get_list_of_sentences_labels(self):\n",
    "        \"\"\"\n",
    "        return : list of sentences : ['I have apple', 'I am here', 'hello ']\n",
    "        return : list of labels : ['O', 'O', 'B-GPE', ...]\n",
    "        \"\"\"\n",
    "        labels, label, sentences, sentence, flat_labels = [], [], [], [], []\n",
    "        for line in self.file:\n",
    "            if line.startswith(\"idx\") or line.startswith(\"0\") or line.startswith(\"NONE\"):\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            splits = line.split(\"\\t\")\n",
    "            if '?' in splits[2] or '.' in splits[2] :\n",
    "                if len(label)>0 and len(sentence)>0:\n",
    "                    sentences.append(\" \".join(sentence))\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "                continue\n",
    "            if splits[3] != 'NONE':\n",
    "                sentence.append(splits[3])\n",
    "                label.append(splits[6])\n",
    "                flat_labels.append(splits[6])\n",
    "\n",
    "        if len(label)>0 and len(sentence)>0:\n",
    "            sentences.append(\" \".join(sentence))\n",
    "            labels.append(label)\n",
    "\n",
    "        labels = [list(map(lambda x: x if x != 'NONE' else 'O', i)) for i in labels]\n",
    "        self.ners_vals = list(map(lambda x: x if x != 'NONE' else 'O', set(flat_labels)))\n",
    "        \n",
    "        print(\"number of sentences:\", len(sentences))\n",
    "        print('num of tags :', len(self.ners_vals))\n",
    "\n",
    "        return sentences, labels\n",
    "\n",
    "    def get_tag2idx_idx2tag(self):\n",
    "        \"\"\"\n",
    "        return : dict of tag2idx : {'B-ADD': 0, 'B-AGE': 1, 'B-ART': 2, 'B-CARDINAL': 3,'B-CREAT': 4, ...}\n",
    "        return : dict of idx2tag : inverted\n",
    "        \"\"\"\n",
    "        tag2idx = {t: i for i, t in enumerate(sorted(self.ners_vals))}\n",
    "        idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "        return tag2idx, idx2tag\n",
    "\n",
    "\n",
    "class TweetPreprocessor:\n",
    "    def __init__(self, filename='data/merged_headlines_annos.compact.tsv'):\n",
    "        print(' ------ Preprocssing Tweets corpus ------')\n",
    "        self.file = open(filename, encoding='utf-8')\n",
    "        self.ners_vals=[]\n",
    "\n",
    "    def get_list_of_sentences_labels(self):\n",
    "        \"\"\"\n",
    "        return : list of sentences : ['I have apple', 'I am here', 'hello ']\n",
    "        return : list of labels : ['O', 'O', 'B-GPE', ...]\n",
    "        \"\"\"\n",
    "        labels, label, sentences, sentence, flat_labels = [], [], [], [], []\n",
    "        for line in self.file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            splits = line.split(\"\\t\")\n",
    "            if line.startswith(\"NONE\"):\n",
    "                if len(label)>0 and len(sentence)>0:\n",
    "                    sentences.append(\" \".join(sentence))\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "                continue\n",
    "            sentence.append(splits[1])\n",
    "            label.append(splits[3])\n",
    "            flat_labels.append(splits[3])\n",
    "        \n",
    "        if len(label)>0 and len(sentence)>0:\n",
    "            sentences.append(\" \".join(sentence))\n",
    "            labels.append(label)\n",
    "            \n",
    "        labels = [list(map(lambda x: x if x != 'NONE' else 'O', i)) for i in labels]\n",
    "        self.ners_vals = list(map(lambda x: x if x != 'NONE' else 'O', set(flat_labels)))\n",
    "        print(\"Total number of tweets\", len(sentences))\n",
    "        print(\"Total number of ner tags in tweets\", len(self.ners_vals))\n",
    "\n",
    "        return sentences, labels\n",
    "\n",
    "    def get_tag2idx_idx2tag(self):\n",
    "        \"\"\"\n",
    "        return : dict of tag2idx : {'B-ADD': 0, 'B-AGE': 1, 'B-ART': 2, 'B-CARDINAL': 3,'B-CREAT': 4, ...}\n",
    "        return : dict of idx2tag : inverted\n",
    "        \"\"\"\n",
    "        tag2idx = {t: i for i, t in enumerate(sorted(self.ners_vals))}\n",
    "        idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "        return tag2idx, idx2tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For ISW data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Preprocssing ISW German corpus ------\n",
      "number of sentences: 16084\n",
      "num of tags : 60\n",
      "number of sentences: 16084\n",
      "num of tags : 60\n",
      "Meine Eindr√ºcke von Wien von damals\n",
      "['O', 'O', 'O', 'B-GPE', 'O', 'B-TIME']\n",
      "{'B-ADD': 0, 'B-AGE': 1, 'B-ART': 2, 'B-CARDINAL': 3, 'B-CREAT': 4, 'B-DATE': 5, 'B-DUR': 6, 'B-EVT': 7, 'B-FAC': 8, 'B-FRAC': 9, 'B-FREQ': 10, 'B-GPE': 11, 'B-LAN': 12, 'B-LAW': 13, 'B-LOC': 14, 'B-MED': 15, 'B-MISC': 16, 'B-MON': 17, 'B-NRP': 18, 'B-ORDINAL': 19, 'B-ORG': 20, 'B-PER': 21, 'B-PERC': 22, 'B-PRODUCT': 23, 'B-PROJ': 24, 'B-QUANT': 25, 'B-RATE': 26, 'B-SORD': 27, 'B-TIME': 28, 'B-TITLE': 29, 'I-ADD': 30, 'I-AGE': 31, 'I-ART': 32, 'I-CARDINAL': 33, 'I-DATE': 34, 'I-DUR': 35, 'I-EVT': 36, 'I-FAC': 37, 'I-FRAC': 38, 'I-FREQ': 39, 'I-GPE': 40, 'I-LAN': 41, 'I-LAW': 42, 'I-LOC': 43, 'I-MED': 44, 'I-MISC': 45, 'I-MON': 46, 'I-NRP': 47, 'I-ORDINAL': 48, 'I-ORG': 49, 'I-PER': 50, 'I-PERC': 51, 'I-PRODUCT': 52, 'I-PROJ': 53, 'I-QUANT': 54, 'I-RATE': 55, 'I-SORD': 56, 'I-TIME': 57, 'I-TITLE': 58, 'O': 59}\n",
      "{0: 'B-ADD', 1: 'B-AGE', 2: 'B-ART', 3: 'B-CARDINAL', 4: 'B-CREAT', 5: 'B-DATE', 6: 'B-DUR', 7: 'B-EVT', 8: 'B-FAC', 9: 'B-FRAC', 10: 'B-FREQ', 11: 'B-GPE', 12: 'B-LAN', 13: 'B-LAW', 14: 'B-LOC', 15: 'B-MED', 16: 'B-MISC', 17: 'B-MON', 18: 'B-NRP', 19: 'B-ORDINAL', 20: 'B-ORG', 21: 'B-PER', 22: 'B-PERC', 23: 'B-PRODUCT', 24: 'B-PROJ', 25: 'B-QUANT', 26: 'B-RATE', 27: 'B-SORD', 28: 'B-TIME', 29: 'B-TITLE', 30: 'I-ADD', 31: 'I-AGE', 32: 'I-ART', 33: 'I-CARDINAL', 34: 'I-DATE', 35: 'I-DUR', 36: 'I-EVT', 37: 'I-FAC', 38: 'I-FRAC', 39: 'I-FREQ', 40: 'I-GPE', 41: 'I-LAN', 42: 'I-LAW', 43: 'I-LOC', 44: 'I-MED', 45: 'I-MISC', 46: 'I-MON', 47: 'I-NRP', 48: 'I-ORDINAL', 49: 'I-ORG', 50: 'I-PER', 51: 'I-PERC', 52: 'I-PRODUCT', 53: 'I-PROJ', 54: 'I-QUANT', 55: 'I-RATE', 56: 'I-SORD', 57: 'I-TIME', 58: 'I-TITLE', 59: 'O'}\n",
      "['I-SORD', 'B-NRP', 'I-ADD', 'B-FREQ', 'B-DATE', 'B-TITLE', 'B-AGE', 'I-LAW', 'B-LOC', 'B-PER', 'B-MON', 'B-PRODUCT', 'B-GPE', 'B-ART', 'B-MISC', 'I-MISC', 'I-ORG', 'I-FRAC', 'B-SORD', 'I-CARDINAL', 'I-PER', 'I-DUR', 'I-NRP', 'I-ART', 'B-ORDINAL', 'B-CARDINAL', 'I-EVT', 'I-PROJ', 'O', 'I-GPE', 'B-QUANT', 'B-MED', 'I-MON', 'B-ADD', 'B-LAW', 'I-PERC', 'B-EVT', 'B-RATE', 'B-FRAC', 'B-PROJ', 'I-AGE', 'I-LAN', 'I-FAC', 'B-LAN', 'I-TITLE', 'B-TIME', 'I-ORDINAL', 'B-FAC', 'B-ORG', 'I-QUANT', 'I-FREQ', 'I-DATE', 'B-DUR', 'I-PRODUCT', 'I-LOC', 'I-RATE', 'B-PERC', 'B-CREAT', 'I-TIME', 'I-MED']\n"
     ]
    }
   ],
   "source": [
    "filename='data/full-isw-release.tsv'\n",
    "isw_pre = IswPreprocessor(filename)\n",
    "sentences, labels = isw_pre.get_list_of_sentences_labels()\n",
    "tag2idx, idx2tag = isw_pre.get_tag2idx_idx2tag()\n",
    "\n",
    "print(\"number of sentences:\", len(sentences))\n",
    "print('num of tags :', len(isw_pre.ners_vals))\n",
    "i = 1\n",
    "print(sentences[i])\n",
    "print(labels[i])\n",
    "print(tag2idx)\n",
    "print(idx2tag)\n",
    "print(isw_pre.ners_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Preprocssing Tweets corpus ------\n",
      "Total number of tweets 8957\n",
      "Total number of ner tags in tweets 63\n",
      "number of sentences: 8957\n",
      "num of tags : 63\n",
      "Wann auch immer der #Brexit kommen mag - die #IHK bereitet die Unternehmen in #Rheinhessen darauf vor und empfiehlt , vom Worst-Case-Szenario auszugehen . [ plus-Inhalt ]\n",
      "['O', 'O', 'O', 'O', 'B-EVT', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "{'B-AGE': 0, 'B-ART': 1, 'B-CARDINAL': 2, 'B-CREAT': 3, 'B-DATE': 4, 'B-DUR': 5, 'B-EVT': 6, 'B-FAC': 7, 'B-FRAC': 8, 'B-FREQ': 9, 'B-GPE': 10, 'B-LAN': 11, 'B-LAW': 12, 'B-LOC': 13, 'B-MED': 14, 'B-MISC': 15, 'B-MON': 16, 'B-NRP': 17, 'B-ORDINAL': 18, 'B-ORG': 19, 'B-PER': 20, 'B-PERC': 21, 'B-PRODUCT': 22, 'B-PROJ': 23, 'B-QUANT': 24, 'B-RATE': 25, 'B-SCORE': 26, 'B-SORD': 27, 'B-TIME': 28, 'B-TITLE': 29, 'B-URL': 30, 'I-AGE': 31, 'I-ART': 32, 'I-CARDINAL': 33, 'I-CREAT': 34, 'I-DATE': 35, 'I-DUR': 36, 'I-EVT': 37, 'I-FAC': 38, 'I-FRAC': 39, 'I-FREQ': 40, 'I-GPE': 41, 'I-LAN': 42, 'I-LAW': 43, 'I-LOC': 44, 'I-MED': 45, 'I-MISC': 46, 'I-MON': 47, 'I-NRP': 48, 'I-ORDINAL': 49, 'I-ORG': 50, 'I-PER': 51, 'I-PERC': 52, 'I-PRODUCT': 53, 'I-PROJ': 54, 'I-QUANT': 55, 'I-RATE': 56, 'I-SCORE': 57, 'I-SORD': 58, 'I-TIME': 59, 'I-TITLE': 60, 'I-URL': 61, 'O': 62}\n",
      "{0: 'B-AGE', 1: 'B-ART', 2: 'B-CARDINAL', 3: 'B-CREAT', 4: 'B-DATE', 5: 'B-DUR', 6: 'B-EVT', 7: 'B-FAC', 8: 'B-FRAC', 9: 'B-FREQ', 10: 'B-GPE', 11: 'B-LAN', 12: 'B-LAW', 13: 'B-LOC', 14: 'B-MED', 15: 'B-MISC', 16: 'B-MON', 17: 'B-NRP', 18: 'B-ORDINAL', 19: 'B-ORG', 20: 'B-PER', 21: 'B-PERC', 22: 'B-PRODUCT', 23: 'B-PROJ', 24: 'B-QUANT', 25: 'B-RATE', 26: 'B-SCORE', 27: 'B-SORD', 28: 'B-TIME', 29: 'B-TITLE', 30: 'B-URL', 31: 'I-AGE', 32: 'I-ART', 33: 'I-CARDINAL', 34: 'I-CREAT', 35: 'I-DATE', 36: 'I-DUR', 37: 'I-EVT', 38: 'I-FAC', 39: 'I-FRAC', 40: 'I-FREQ', 41: 'I-GPE', 42: 'I-LAN', 43: 'I-LAW', 44: 'I-LOC', 45: 'I-MED', 46: 'I-MISC', 47: 'I-MON', 48: 'I-NRP', 49: 'I-ORDINAL', 50: 'I-ORG', 51: 'I-PER', 52: 'I-PERC', 53: 'I-PRODUCT', 54: 'I-PROJ', 55: 'I-QUANT', 56: 'I-RATE', 57: 'I-SCORE', 58: 'I-SORD', 59: 'I-TIME', 60: 'I-TITLE', 61: 'I-URL', 62: 'O'}\n",
      "['I-SORD', 'B-NRP', 'B-FREQ', 'B-DATE', 'I-SCORE', 'I-CREAT', 'B-TITLE', 'B-AGE', 'I-LAW', 'B-LOC', 'B-URL', 'B-PER', 'B-MON', 'B-PRODUCT', 'B-GPE', 'B-ART', 'B-MISC', 'I-MISC', 'I-ORG', 'I-FRAC', 'B-SCORE', 'B-SORD', 'I-CARDINAL', 'I-PER', 'I-DUR', 'I-NRP', 'I-ART', 'I-EVT', 'B-CARDINAL', 'I-PROJ', 'B-ORDINAL', 'O', 'I-GPE', 'B-QUANT', 'I-MON', 'B-MED', 'B-LAW', 'I-PERC', 'B-EVT', 'B-RATE', 'B-FRAC', 'B-PROJ', 'I-AGE', 'I-LAN', 'I-TITLE', 'I-FAC', 'B-LAN', 'B-TIME', 'I-MED', 'I-ORDINAL', 'B-FAC', 'B-ORG', 'I-QUANT', 'I-DATE', 'I-FREQ', 'B-DUR', 'I-PRODUCT', 'I-LOC', 'I-URL', 'I-RATE', 'B-PERC', 'I-TIME', 'B-CREAT']\n"
     ]
    }
   ],
   "source": [
    "filename='data/merged_headlines_annos.compact.tsv'\n",
    "\n",
    "tweet_pre = TweetPreprocessor(filename)\n",
    "sentences, labels = tweet_pre.get_list_of_sentences_labels()\n",
    "tag2idx, idx2tag = tweet_pre.get_tag2idx_idx2tag()\n",
    "\n",
    "print(\"number of sentences:\", len(sentences))\n",
    "print('num of tags :', len(tweet_pre.ners_vals))\n",
    "i = 2\n",
    "print(sentences[i])\n",
    "print(labels[i])\n",
    "print(tag2idx)\n",
    "print(idx2tag)\n",
    "print(tweet_pre.ners_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-SCORE', 'B-URL', 'I-CREAT', 'I-SCORE', 'I-URL'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tweet_pre.ners_vals)-set(isw_pre.ners_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ADD', 'I-ADD'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(isw_pre.ners_vals)-set(tweet_pre.ners_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'Also', 'Sie', 'haben', 'uns', 'ja', 'jetzt'],\n",
       " ['Die', 'zwei'],\n",
       " ['Jetzt'],\n",
       " ['Ja', 'dann'],\n",
       " ['nach'],\n",
       " ['Nach'],\n",
       " ['Jetzt', 'Jetzt']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\", do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "tokenized_texts = [[ll for ll in e if \"##\" not in ll] for e in tokenized_texts ]\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'O', 'O', 'O', 'O', 'O', 'B-TIME'],\n",
       " ['O', 'B-CARDINAL'],\n",
       " ['B-TIME'],\n",
       " ['O', 'B-TIME'],\n",
       " ['B-TIME'],\n",
       " ['B-TIME'],\n",
       " ['B-TIME', 'B-TIME']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  103, 26938, 12482,   371,   474,  2099,  3278,  1868],\n",
       "       [  125,   382,     0,     0,     0,     0,     0,     0],\n",
       "       [ 5072,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 6802,   670,     0,     0,     0,     0,     0,     0],\n",
       "       [  188,   320,     0,     0,     0,     0,     0,     0],\n",
       "       [  326,   320,     0,     0,     0,     0,     0,     0],\n",
       "       [ 5072,  5072,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len=9\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "#                                         maxlen=max_len, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 2, 2, 2, 1],\n",
       "       [2, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "#                                         maxlen=max_len,  \n",
    "                                         padding=\"post\",\n",
    "                                        dtype=\"long\", truncating=\"post\")\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from preprocessor.preprocessor import *\n",
    "from transformers import BertTokenizer, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ner:\n",
    "    def __init__(self,model_dir: str):\n",
    "        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n",
    "        self.label_map = self.model_config[\"label_map\"]\n",
    "        self.max_seq_length = self.model_config[\"max_seq_length\"]\n",
    "        self.max_seq_length = 10\n",
    "        self.label_map = {int(k):v for k,v in self.label_map.items()}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self, model_dir: str, model_config: str = \"model_config.json\"):\n",
    "        model_config = os.path.join(model_dir,model_config)\n",
    "        model_config = json.load(open(model_config))\n",
    "#         model = BertForTokenClassification.from_pretrained(model_dir)\n",
    "#         tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "        model = BertForTokenClassification.from_pretrained('bert-base-uncased')\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        return model, tokenizer, model_config\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\" tokenize input\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        tokens = []\n",
    "        valid_positions = []\n",
    "        for i,word in enumerate(words):\n",
    "            token = self.tokenizer.tokenize(word)\n",
    "            token = [ t for t in token if \"##\" not in t ]\n",
    "            tokens.extend(token)\n",
    "            for i in range(len(token)):\n",
    "                if i == 0:\n",
    "                    valid_positions.append(1)\n",
    "                else:\n",
    "                    valid_positions.append(0)\n",
    "        return tokens, valid_positions\n",
    "\n",
    "    def preprocess(self, text: str):\n",
    "        \"\"\" preprocess \"\"\"\n",
    "        tokens, valid_positions = self.tokenize(text)\n",
    "        segment_ids = []\n",
    "        for i in range(len(tokens)):\n",
    "            segment_ids.append(0)\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        while len(input_ids) < self.max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            valid_positions.append(0)\n",
    "        return input_ids,input_mask,segment_ids,valid_positions\n",
    "\n",
    "    def predict(self, text: str):\n",
    "        input_ids,input_mask,segment_ids,valid_ids = self.preprocess(text)\n",
    "        input_ids = torch.tensor([input_ids],dtype=torch.long,device=self.device)\n",
    "        input_mask = torch.tensor([input_mask],dtype=torch.long,device=self.device)\n",
    "        segment_ids = torch.tensor([segment_ids],dtype=torch.long,device=self.device)\n",
    "        valid_ids = torch.tensor([valid_ids],dtype=torch.long,device=self.device)\n",
    "#         print('input_ids', input_ids)\n",
    "#         print('input_mask', input_mask)\n",
    "#         print('segment_ids', segment_ids)\n",
    "#         print('valid_ids', valid_ids)\n",
    "#         print('valid_ids[0]', valid_ids[0])\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, segment_ids, input_mask,valid_ids)\n",
    "            logits = outputs[0]\n",
    "#             print('logit type', type(logits))\n",
    "#             print('logit ', logits)\n",
    "        logits = F.softmax(logits,dim=2)\n",
    "        logits_label = torch.argmax(logits,dim=2)\n",
    "        logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n",
    "#         print('logits_label:', logits_label)\n",
    "        logits_confidence = [values[label].item() for values,label in zip(logits[0],logits_label)]\n",
    "#         print('logits_confidence', logits_confidence)\n",
    "\n",
    "        logits = []\n",
    "        for index,mask in enumerate(valid_ids[0]):\n",
    "            if mask == 1:\n",
    "#                 print('hi', mask)\n",
    "                logits.append((logits_label[index], logits_confidence[index]))\n",
    "#                 print('app logits', logits)\n",
    "            else:\n",
    "                pass\n",
    "        print('label_map', self.label_map)\n",
    "#         print('logit.pop', logits)\n",
    "        labels = [(self.label_map[label],confidence) for label,confidence in logits]\n",
    "        words = word_tokenize(text)\n",
    "#         print('words:', words)\n",
    "#         print('labels:', labels)\n",
    "        assert len(labels) == len(words)\n",
    "        output = [{\"word\":word,\"tag\":label,\"confidence\":confidence} for word,(label,confidence) in zip(words,labels)]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences ='jetzt bin ich zw√∂lf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jetzt bin ich zw√∂lf'"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6031a07a42334265916605ed5e000f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-424-6288ee0b3d15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/steve.chen/thesis/thesis-ner-co-tri-training/models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-421-81318b19cb62>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_seq_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-421-81318b19cb62>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_dir, model_config)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         model = BertForTokenClassification.from_pretrained(model_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         tokenizer = BertTokenizer.from_pretrained(model_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                     \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m                 )\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         )\n\u001b[1;32m    240\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0ms3_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storing %s in cache at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, user_agent)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     )\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m                 if (\n\u001b[1;32m    509\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ner = Ner(\"/Users/steve.chen/thesis/thesis-ner-co-tri-training/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jetzt', 'bin', 'ich', 'zw√∂lf']\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokens, valid_positions = ner.tokenize(sentences)\n",
    "print(tokens)\n",
    "print(valid_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids,input_mask,segment_ids,valid_positions = ner.preprocess(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1868, 4058, 1169, 4420, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(input_mask)\n",
    "print(segment_ids)\n",
    "print(valid_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_map {0: 'B-ADD', 1: 'B-AGE', 2: 'B-CARDINAL', 3: 'B-DATE', 4: 'B-DUR', 5: 'B-FAC', 6: 'B-FREQ', 7: 'B-GPE', 8: 'B-NRP', 9: 'B-PER', 10: 'B-SORD', 11: 'B-TIME', 12: 'B-TITLE', 13: 'I-ADD', 14: 'I-GPE', 15: 'I-PER', 16: 'I-SORD', 17: 'O'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'jetzt', 'tag': 'B-ADD', 'confidence': 0.6784576177597046},\n",
       " {'word': 'bin', 'tag': 'B-ADD', 'confidence': 0.5905336737632751},\n",
       " {'word': 'ich', 'tag': 'B-ADD', 'confidence': 0.7070490717887878},\n",
       " {'word': 'zw√∂lf', 'tag': 'B-ADD', 'confidence': 0.6636413931846619}]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[1169, 1631,  188,    0,    0,    0,    0,    0,    0,    0]])\n",
      "input_mask tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
      "segment_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "valid_ids tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
      "valid_ids[0] tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "logit type <class 'torch.Tensor'>\n",
      "logit  tensor([[[-0.1963,  0.8812, -0.6778],\n",
      "         [-0.1679,  0.6354, -0.4804],\n",
      "         [-0.4352,  0.9377, -0.6370],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636]]])\n",
      "logits_label: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "logits_confidence [0.6448203325271606, 0.5632150173187256, 0.684721052646637, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963]\n",
      "0 tensor(1)\n",
      "hi tensor(1)\n",
      "app logits [(1, 0.6448203325271606)]\n",
      "1 tensor(1)\n",
      "hi tensor(1)\n",
      "app logits [(1, 0.6448203325271606), (1, 0.5632150173187256)]\n",
      "2 tensor(1)\n",
      "hi tensor(1)\n",
      "app logits [(1, 0.6448203325271606), (1, 0.5632150173187256), (1, 0.684721052646637)]\n",
      "3 tensor(0)\n",
      "4 tensor(0)\n",
      "5 tensor(0)\n",
      "6 tensor(0)\n",
      "7 tensor(0)\n",
      "8 tensor(0)\n",
      "9 tensor(0)\n",
      "label_map {0: 'B-CARDINAL', 1: 'B-TIME', 2: 'O'}\n",
      "logit.pop [(1, 0.6448203325271606), (1, 0.5632150173187256), (1, 0.684721052646637)]\n",
      "words: ['ich', 'werde', 'nacher']\n",
      "labels: [('B-TIME', 0.6448203325271606), ('B-TIME', 0.5632150173187256), ('B-TIME', 0.684721052646637)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'ich', 'tag': 'B-TIME', 'confidence': 0.6448203325271606},\n",
       " {'word': 'werde', 'tag': 'B-TIME', 'confidence': 0.5632150173187256},\n",
       " {'word': 'nacher', 'tag': 'B-TIME', 'confidence': 0.684721052646637}]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences ='jetzt bin ich zw√∂lf'\n",
    "ner.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
