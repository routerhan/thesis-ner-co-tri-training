{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class IswPreprocessor:\n",
    "    def __init__(self, filename):\n",
    "        print(' ------ Preprocssing ISW German corpus ------')\n",
    "        self.file = self.load_isw_tsv_file(filename)\n",
    "        self.ners_vals=[]\n",
    "\n",
    "    def load_isw_tsv_file(self, filename='data/test-full-isw-release.tsv'):\n",
    "        file = open(filename, encoding='utf-8')\n",
    "        return file\n",
    "\n",
    "    def get_list_of_sentences_labels(self):\n",
    "        \"\"\"\n",
    "        return : list of sentences : ['I have apple', 'I am here', 'hello ']\n",
    "        return : list of labels : ['O', 'O', 'B-GPE', ...]\n",
    "        \"\"\"\n",
    "        labels, label, sentences, sentence, flat_labels = [], [], [], [], []\n",
    "        for line in self.file:\n",
    "            if line.startswith(\"idx\") or line.startswith(\"0\") or line.startswith(\"NONE\"):\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            splits = line.split(\"\\t\")\n",
    "            if '?' in splits[2] or '.' in splits[2] :\n",
    "                if len(label)>0 and len(sentence)>0:\n",
    "                    sentences.append(\" \".join(sentence))\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "                continue\n",
    "            if splits[3] != 'NONE':\n",
    "                sentence.append(splits[3])\n",
    "                label.append(splits[6])\n",
    "                flat_labels.append(splits[6])\n",
    "\n",
    "        if len(label)>0 and len(sentence)>0:\n",
    "            sentences.append(\" \".join(sentence))\n",
    "            labels.append(label)\n",
    "\n",
    "        labels = [list(map(lambda x: x if x != 'NONE' else 'O', i)) for i in labels]\n",
    "        self.ners_vals = list(map(lambda x: x if x != 'NONE' else 'O', set(flat_labels)))\n",
    "        \n",
    "        print(\"number of sentences:\", len(sentences))\n",
    "        print('num of tags :', len(ners_vals))\n",
    "\n",
    "        return sentences, labels\n",
    "\n",
    "    def get_tag2idx_idx2tag(self):\n",
    "        \"\"\"\n",
    "        return : dict of tag2idx : {'B-ADD': 0, 'B-AGE': 1, 'B-ART': 2, 'B-CARDINAL': 3,'B-CREAT': 4, ...}\n",
    "        return : dict of idx2tag : inverted\n",
    "        \"\"\"\n",
    "        tag2idx = {t: i for i, t in enumerate(sorted(self.ners_vals))}\n",
    "        idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "        return tag2idx, idx2tag\n",
    "\n",
    "\n",
    "class TweetPreprocessor:\n",
    "    def __init__(self, filename='data/merged_headlines_annos.compact.tsv'):\n",
    "        print(' ------ Preprocssing Tweets corpus ------')\n",
    "        self.file = open(filename, encoding='utf-8')\n",
    "        self.ners_vals=[]\n",
    "\n",
    "    def get_list_of_sentences_labels(self):\n",
    "        \"\"\"\n",
    "        return : list of sentences : ['I have apple', 'I am here', 'hello ']\n",
    "        return : list of labels : ['O', 'O', 'B-GPE', ...]\n",
    "        \"\"\"\n",
    "        labels, label, sentences, sentence, flat_labels = [], [], [], [], []\n",
    "        for line in self.file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            splits = line.split(\"\\t\")\n",
    "            if line.startswith(\"NONE\"):\n",
    "                if len(label)>0 and len(sentence)>0:\n",
    "                    sentences.append(\" \".join(sentence))\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "                continue\n",
    "            sentence.append(splits[1])\n",
    "            label.append(splits[3])\n",
    "            flat_labels.append(splits[3])\n",
    "        \n",
    "        if len(label)>0 and len(sentence)>0:\n",
    "            sentences.append(\" \".join(sentence))\n",
    "            labels.append(label)\n",
    "            \n",
    "        labels = [list(map(lambda x: x if x != 'NONE' else 'O', i)) for i in labels]\n",
    "        self.ners_vals = list(map(lambda x: x if x != 'NONE' else 'O', set(flat_labels)))\n",
    "        print(\"Total number of tweets\", len(sentences))\n",
    "        print(\"Total number of ner tags in tweets\", len(self.ners_vals))\n",
    "\n",
    "        return sentences, labels\n",
    "\n",
    "    def get_tag2idx_idx2tag(self):\n",
    "        \"\"\"\n",
    "        return : dict of tag2idx : {'B-ADD': 0, 'B-AGE': 1, 'B-ART': 2, 'B-CARDINAL': 3,'B-CREAT': 4, ...}\n",
    "        return : dict of idx2tag : inverted\n",
    "        \"\"\"\n",
    "        tag2idx = {t: i for i, t in enumerate(sorted(self.ners_vals))}\n",
    "        idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "        return tag2idx, idx2tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For ISW data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Preprocssing ISW German corpus ------\n",
      "number of sentences: 16084\n",
      "num of tags : 61\n"
     ]
    }
   ],
   "source": [
    "filename='data/full-isw-release.tsv'\n",
    "isw_pre = IswPreprocessor(filename)\n",
    "sentences, labels = isw_pre.get_list_of_sentences_labels()\n",
    "tag2idx, idx2tag = isw_pre.get_tag2idx_idx2tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences: 16084\n",
      "num of tags : 60\n",
      "Meine EindrÃ¼cke von Wien von damals\n",
      "['O', 'O', 'O', 'B-GPE', 'O', 'B-TIME']\n",
      "{'B-ADD': 0, 'B-AGE': 1, 'B-ART': 2, 'B-CARDINAL': 3, 'B-CREAT': 4, 'B-DATE': 5, 'B-DUR': 6, 'B-EVT': 7, 'B-FAC': 8, 'B-FRAC': 9, 'B-FREQ': 10, 'B-GPE': 11, 'B-LAN': 12, 'B-LAW': 13, 'B-LOC': 14, 'B-MED': 15, 'B-MISC': 16, 'B-MON': 17, 'B-NRP': 18, 'B-ORDINAL': 19, 'B-ORG': 20, 'B-PER': 21, 'B-PERC': 22, 'B-PRODUCT': 23, 'B-PROJ': 24, 'B-QUANT': 25, 'B-RATE': 26, 'B-SORD': 27, 'B-TIME': 28, 'B-TITLE': 29, 'I-ADD': 30, 'I-AGE': 31, 'I-ART': 32, 'I-CARDINAL': 33, 'I-DATE': 34, 'I-DUR': 35, 'I-EVT': 36, 'I-FAC': 37, 'I-FRAC': 38, 'I-FREQ': 39, 'I-GPE': 40, 'I-LAN': 41, 'I-LAW': 42, 'I-LOC': 43, 'I-MED': 44, 'I-MISC': 45, 'I-MON': 46, 'I-NRP': 47, 'I-ORDINAL': 48, 'I-ORG': 49, 'I-PER': 50, 'I-PERC': 51, 'I-PRODUCT': 52, 'I-PROJ': 53, 'I-QUANT': 54, 'I-RATE': 55, 'I-SORD': 56, 'I-TIME': 57, 'I-TITLE': 58, 'O': 59}\n",
      "{0: 'B-ADD', 1: 'B-AGE', 2: 'B-ART', 3: 'B-CARDINAL', 4: 'B-CREAT', 5: 'B-DATE', 6: 'B-DUR', 7: 'B-EVT', 8: 'B-FAC', 9: 'B-FRAC', 10: 'B-FREQ', 11: 'B-GPE', 12: 'B-LAN', 13: 'B-LAW', 14: 'B-LOC', 15: 'B-MED', 16: 'B-MISC', 17: 'B-MON', 18: 'B-NRP', 19: 'B-ORDINAL', 20: 'B-ORG', 21: 'B-PER', 22: 'B-PERC', 23: 'B-PRODUCT', 24: 'B-PROJ', 25: 'B-QUANT', 26: 'B-RATE', 27: 'B-SORD', 28: 'B-TIME', 29: 'B-TITLE', 30: 'I-ADD', 31: 'I-AGE', 32: 'I-ART', 33: 'I-CARDINAL', 34: 'I-DATE', 35: 'I-DUR', 36: 'I-EVT', 37: 'I-FAC', 38: 'I-FRAC', 39: 'I-FREQ', 40: 'I-GPE', 41: 'I-LAN', 42: 'I-LAW', 43: 'I-LOC', 44: 'I-MED', 45: 'I-MISC', 46: 'I-MON', 47: 'I-NRP', 48: 'I-ORDINAL', 49: 'I-ORG', 50: 'I-PER', 51: 'I-PERC', 52: 'I-PRODUCT', 53: 'I-PROJ', 54: 'I-QUANT', 55: 'I-RATE', 56: 'I-SORD', 57: 'I-TIME', 58: 'I-TITLE', 59: 'O'}\n",
      "['I-SORD', 'B-NRP', 'I-ADD', 'B-FREQ', 'B-DATE', 'B-TITLE', 'B-AGE', 'I-LAW', 'B-LOC', 'B-PER', 'B-MON', 'B-PRODUCT', 'B-GPE', 'B-ART', 'B-MISC', 'I-MISC', 'I-ORG', 'I-FRAC', 'B-SORD', 'I-CARDINAL', 'I-PER', 'I-DUR', 'I-NRP', 'I-ART', 'B-ORDINAL', 'B-CARDINAL', 'I-EVT', 'I-PROJ', 'O', 'I-GPE', 'B-QUANT', 'B-MED', 'I-MON', 'B-ADD', 'B-LAW', 'I-PERC', 'B-EVT', 'B-RATE', 'B-FRAC', 'B-PROJ', 'I-AGE', 'I-LAN', 'I-FAC', 'B-LAN', 'I-TITLE', 'B-TIME', 'I-ORDINAL', 'B-FAC', 'B-ORG', 'I-QUANT', 'I-FREQ', 'I-DATE', 'B-DUR', 'I-PRODUCT', 'I-LOC', 'I-RATE', 'B-PERC', 'B-CREAT', 'I-TIME', 'I-MED']\n"
     ]
    }
   ],
   "source": [
    "print(\"number of sentences:\", len(sentences))\n",
    "print('num of tags :', len(isw_pre.ners_vals))\n",
    "i = 1\n",
    "print(sentences[i])\n",
    "print(labels[i])\n",
    "print(tag2idx)\n",
    "print(idx2tag)\n",
    "print(isw_pre.ners_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Preprocssing Tweets corpus ------\n",
      "Total number of tweets 8957\n",
      "Total number of ner tags in tweets 63\n",
      "8957\n",
      "8957\n",
      "Moin , Axel !\n",
      "['O', 'O', 'B-PER', 'O']\n",
      "{'B-AGE': 0, 'B-ART': 1, 'B-CARDINAL': 2, 'B-CREAT': 3, 'B-DATE': 4, 'B-DUR': 5, 'B-EVT': 6, 'B-FAC': 7, 'B-FRAC': 8, 'B-FREQ': 9, 'B-GPE': 10, 'B-LAN': 11, 'B-LAW': 12, 'B-LOC': 13, 'B-MED': 14, 'B-MISC': 15, 'B-MON': 16, 'B-NRP': 17, 'B-ORDINAL': 18, 'B-ORG': 19, 'B-PER': 20, 'B-PERC': 21, 'B-PRODUCT': 22, 'B-PROJ': 23, 'B-QUANT': 24, 'B-RATE': 25, 'B-SCORE': 26, 'B-SORD': 27, 'B-TIME': 28, 'B-TITLE': 29, 'B-URL': 30, 'I-AGE': 31, 'I-ART': 32, 'I-CARDINAL': 33, 'I-CREAT': 34, 'I-DATE': 35, 'I-DUR': 36, 'I-EVT': 37, 'I-FAC': 38, 'I-FRAC': 39, 'I-FREQ': 40, 'I-GPE': 41, 'I-LAN': 42, 'I-LAW': 43, 'I-LOC': 44, 'I-MED': 45, 'I-MISC': 46, 'I-MON': 47, 'I-NRP': 48, 'I-ORDINAL': 49, 'I-ORG': 50, 'I-PER': 51, 'I-PERC': 52, 'I-PRODUCT': 53, 'I-PROJ': 54, 'I-QUANT': 55, 'I-RATE': 56, 'I-SCORE': 57, 'I-SORD': 58, 'I-TIME': 59, 'I-TITLE': 60, 'I-URL': 61, 'O': 62}\n",
      "{0: 'B-AGE', 1: 'B-ART', 2: 'B-CARDINAL', 3: 'B-CREAT', 4: 'B-DATE', 5: 'B-DUR', 6: 'B-EVT', 7: 'B-FAC', 8: 'B-FRAC', 9: 'B-FREQ', 10: 'B-GPE', 11: 'B-LAN', 12: 'B-LAW', 13: 'B-LOC', 14: 'B-MED', 15: 'B-MISC', 16: 'B-MON', 17: 'B-NRP', 18: 'B-ORDINAL', 19: 'B-ORG', 20: 'B-PER', 21: 'B-PERC', 22: 'B-PRODUCT', 23: 'B-PROJ', 24: 'B-QUANT', 25: 'B-RATE', 26: 'B-SCORE', 27: 'B-SORD', 28: 'B-TIME', 29: 'B-TITLE', 30: 'B-URL', 31: 'I-AGE', 32: 'I-ART', 33: 'I-CARDINAL', 34: 'I-CREAT', 35: 'I-DATE', 36: 'I-DUR', 37: 'I-EVT', 38: 'I-FAC', 39: 'I-FRAC', 40: 'I-FREQ', 41: 'I-GPE', 42: 'I-LAN', 43: 'I-LAW', 44: 'I-LOC', 45: 'I-MED', 46: 'I-MISC', 47: 'I-MON', 48: 'I-NRP', 49: 'I-ORDINAL', 50: 'I-ORG', 51: 'I-PER', 52: 'I-PERC', 53: 'I-PRODUCT', 54: 'I-PROJ', 55: 'I-QUANT', 56: 'I-RATE', 57: 'I-SCORE', 58: 'I-SORD', 59: 'I-TIME', 60: 'I-TITLE', 61: 'I-URL', 62: 'O'}\n"
     ]
    }
   ],
   "source": [
    "filename='data/merged_headlines_annos.compact.tsv'\n",
    "\n",
    "tweet_pre = TweetPreprocessor(filename)\n",
    "sentences, labels = tweet_pre.get_list_of_sentences_labels()\n",
    "tag2idx, idx2tag = tweet_pre.get_tag2idx_idx2tag()\n",
    "\n",
    "print(len(labels))\n",
    "print(len(sentences))\n",
    "print(sentences[-1])\n",
    "print(labels[-1])\n",
    "print(tag2idx)\n",
    "print(idx2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-AGE': 0,\n",
       " 'B-ART': 1,\n",
       " 'B-CARDINAL': 2,\n",
       " 'B-CREAT': 3,\n",
       " 'B-DATE': 4,\n",
       " 'B-DUR': 5,\n",
       " 'B-EVT': 6,\n",
       " 'B-FAC': 7,\n",
       " 'B-FRAC': 8,\n",
       " 'B-FREQ': 9,\n",
       " 'B-GPE': 10,\n",
       " 'B-LAN': 11,\n",
       " 'B-LAW': 12,\n",
       " 'B-LOC': 13,\n",
       " 'B-MED': 14,\n",
       " 'B-MISC': 15,\n",
       " 'B-MON': 16,\n",
       " 'B-NRP': 17,\n",
       " 'B-ORDINAL': 18,\n",
       " 'B-ORG': 19,\n",
       " 'B-PER': 20,\n",
       " 'B-PERC': 21,\n",
       " 'B-PRODUCT': 22,\n",
       " 'B-PROJ': 23,\n",
       " 'B-QUANT': 24,\n",
       " 'B-RATE': 25,\n",
       " 'B-SCORE': 26,\n",
       " 'B-SORD': 27,\n",
       " 'B-TIME': 28,\n",
       " 'B-TITLE': 29,\n",
       " 'B-URL': 30,\n",
       " 'I-AGE': 31,\n",
       " 'I-ART': 32,\n",
       " 'I-CARDINAL': 33,\n",
       " 'I-CREAT': 34,\n",
       " 'I-DATE': 35,\n",
       " 'I-DUR': 36,\n",
       " 'I-EVT': 37,\n",
       " 'I-FAC': 38,\n",
       " 'I-FRAC': 39,\n",
       " 'I-FREQ': 40,\n",
       " 'I-GPE': 41,\n",
       " 'I-LAN': 42,\n",
       " 'I-LAW': 43,\n",
       " 'I-LOC': 44,\n",
       " 'I-MED': 45,\n",
       " 'I-MISC': 46,\n",
       " 'I-MON': 47,\n",
       " 'I-NRP': 48,\n",
       " 'I-ORDINAL': 49,\n",
       " 'I-ORG': 50,\n",
       " 'I-PER': 51,\n",
       " 'I-PERC': 52,\n",
       " 'I-PRODUCT': 53,\n",
       " 'I-PROJ': 54,\n",
       " 'I-QUANT': 55,\n",
       " 'I-RATE': 56,\n",
       " 'I-SCORE': 57,\n",
       " 'I-SORD': 58,\n",
       " 'I-TIME': 59,\n",
       " 'I-TITLE': 60,\n",
       " 'I-URL': 61,\n",
       " 'O': 62}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'Also', 'Sie', 'haben', 'uns', 'ja', 'jetzt'],\n",
       " ['Die', 'zwei'],\n",
       " ['Jetzt'],\n",
       " ['Ja', 'dann'],\n",
       " ['nach'],\n",
       " ['Nach'],\n",
       " ['Jetzt', 'Jetzt']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\", do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "tokenized_texts = [[ll for ll in e if \"##\" not in ll] for e in tokenized_texts ]\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'O', 'O', 'O', 'O', 'O', 'B-TIME'],\n",
       " ['O', 'B-CARDINAL'],\n",
       " ['B-TIME'],\n",
       " ['O', 'B-TIME'],\n",
       " ['B-TIME'],\n",
       " ['B-TIME'],\n",
       " ['B-TIME', 'B-TIME']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  103, 26938, 12482,   371,   474,  2099,  3278,  1868],\n",
       "       [  125,   382,     0,     0,     0,     0,     0,     0],\n",
       "       [ 5072,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 6802,   670,     0,     0,     0,     0,     0,     0],\n",
       "       [  188,   320,     0,     0,     0,     0,     0,     0],\n",
       "       [  326,   320,     0,     0,     0,     0,     0,     0],\n",
       "       [ 5072,  5072,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len=9\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "#                                         maxlen=max_len, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 2, 2, 2, 1],\n",
       "       [2, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "#                                         maxlen=max_len,  \n",
    "                                         padding=\"post\",\n",
    "                                        dtype=\"long\", truncating=\"post\")\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from preprocessor.preprocessor import *\n",
    "from transformers import BertTokenizer, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ner:\n",
    "    def __init__(self,model_dir: str):\n",
    "        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n",
    "        self.label_map = self.model_config[\"label_map\"]\n",
    "#         self.max_seq_length = self.model_config[\"max_seq_length\"]\n",
    "        self.max_seq_length = 10\n",
    "        self.label_map = {int(k):v for k,v in self.label_map.items()}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self, model_dir: str, model_config: str = \"model_config.json\"):\n",
    "        model_config = os.path.join(model_dir,model_config)\n",
    "        model_config = json.load(open(model_config))\n",
    "        model = BertForTokenClassification.from_pretrained(model_dir)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "        return model, tokenizer, model_config\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\" tokenize input\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        tokens = []\n",
    "        valid_positions = []\n",
    "        for i,word in enumerate(words):\n",
    "            token = self.tokenizer.tokenize(word)\n",
    "            tokens.extend(token)\n",
    "            for i in range(len(token)):\n",
    "                if i == 0:\n",
    "                    valid_positions.append(1)\n",
    "                else:\n",
    "                    valid_positions.append(0)\n",
    "        return tokens, valid_positions\n",
    "\n",
    "    def preprocess(self, text: str):\n",
    "        \"\"\" preprocess \"\"\"\n",
    "        tokens, valid_positions = self.tokenize(text)\n",
    "        ## insert \"[CLS]\"\n",
    "        tokens.insert(0,\"[CLS]\")\n",
    "        valid_positions.insert(0,1)\n",
    "        ## insert \"[SEP]\"\n",
    "        tokens.append(\"[SEP]\")\n",
    "        valid_positions.append(1)\n",
    "        segment_ids = []\n",
    "        for i in range(len(tokens)):\n",
    "            segment_ids.append(0)\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        while len(input_ids) < self.max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            valid_positions.append(0)\n",
    "        return input_ids,input_mask,segment_ids,valid_positions\n",
    "\n",
    "    def predict(self, text: str):\n",
    "        input_ids,input_mask,segment_ids,valid_ids = self.preprocess(text)\n",
    "        input_ids = torch.tensor([input_ids],dtype=torch.long,device=self.device)\n",
    "        input_mask = torch.tensor([input_mask],dtype=torch.long,device=self.device)\n",
    "        segment_ids = torch.tensor([segment_ids],dtype=torch.long,device=self.device)\n",
    "        valid_ids = torch.tensor([valid_ids],dtype=torch.long,device=self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids, segment_ids, input_mask,valid_ids)\n",
    "            print(logits)\n",
    "            print(type(logits))\n",
    "        logits = F.softmax(logits,dim=2)\n",
    "        logits_label = torch.argmax(logits,dim=2)\n",
    "        logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "        logits_confidence = [values[label].item() for values,label in zip(logits[0],logits_label)]\n",
    "\n",
    "        logits = []\n",
    "        pos = 0\n",
    "        for index,mask in enumerate(valid_ids[0]):\n",
    "            if index == 0:\n",
    "                continue\n",
    "            if mask == 1:\n",
    "                logits.append((logits_label[index-pos],logits_confidence[index-pos]))\n",
    "            else:\n",
    "                pos += 1\n",
    "        logits.pop()\n",
    "\n",
    "        labels = [(self.label_map[label],confidence) for label,confidence in logits]\n",
    "        words = word_tokenize(text)\n",
    "        assert len(labels) == len(words)\n",
    "        output = [{\"word\":word,\"tag\":label,\"confidence\":confidence} for word,(label,confidence) in zip(words,labels)]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences ='IR Also Sie haben uns ja jetzt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IR Also Sie haben uns ja jetzt'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = Ner(\"/Users/steve.chen/thesis/thesis-ner-co-tri-training/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '##R', 'Also', 'Sie', 'haben', 'uns', 'ja', 'jetzt']\n",
      "[1, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokens, valid_positions = ner.tokenize(sentences)\n",
    "print(tokens)\n",
    "print(valid_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids,input_mask,segment_ids,valid_positions = ner.preprocess(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 103, 26938, 12482, 371, 474, 2099, 3278, 1868, 4]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(input_mask)\n",
    "print(segment_ids)\n",
    "print(valid_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.0722,  0.4482, -0.7476],\n",
      "         [-0.2004,  0.3927, -0.3364],\n",
      "         [-0.0870,  0.3878, -0.9592],\n",
      "         [ 0.1453,  0.6092, -0.6444],\n",
      "         [ 0.1490,  0.6808, -0.3033],\n",
      "         [ 0.5548,  0.6449, -0.6449],\n",
      "         [-0.0932,  0.4941, -0.7900],\n",
      "         [-0.4415,  0.4773, -0.7838],\n",
      "         [ 0.3629,  0.4523, -0.3984],\n",
      "         [-0.1987,  0.4722, -1.0635]]]),)\n",
      "<class 'tuple'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-0f739fc05ac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-156-993e3b78ccd4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mlogits_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mlogits_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "ner.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
