{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, valid_ids=None, label_mask=None):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.valid_ids = valid_ids\n",
    "        self.label_mask = label_mask\n",
    "\n",
    "def convert_examples_to_features(all_sentences, all_labels, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"\n",
    "    :input: examples\n",
    "    :return: list of features\n",
    "    :input_ids, input_mask, segment_ids, label_ids, valid_ids, label_mask\n",
    "    \"\"\"\n",
    "    label_map = {label : i for i, label in enumerate(label_list,1)}\n",
    "    print('label_map', label_map)\n",
    "    print('\\n')\n",
    "\n",
    "    features = []\n",
    "    for index, sentence in enumerate(all_sentences):\n",
    "        textlist = sentence.split(' ')\n",
    "        print('textlist', textlist)\n",
    "        labellist = all_labels[index]\n",
    "        print('labellist', labellist)\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        valid = []\n",
    "        label_mask = []\n",
    "        for i, word in enumerate(textlist):\n",
    "            token = tokenizer.tokenize(word)\n",
    "            token = [ t for t in token if \"##\" not in t ]\n",
    "            tokens.extend(token)\n",
    "            label_1 = labellist[i]\n",
    "            for m in range(len(token)):\n",
    "                if m == 0:\n",
    "                    labels.append(label_1)\n",
    "                    valid.append(1)\n",
    "                    label_mask.append(1)\n",
    "                else:\n",
    "                    valid.append(0)\n",
    "        if len(tokens)>= max_seq_length -1 :\n",
    "            tokens = tokens[0:(max_seq_length - 1)]\n",
    "            labels = labels[0:(max_seq_length - 1)]\n",
    "            valid = valid[0:(max_seq_length - 1)]\n",
    "            label_mask = label_mask[0:(max_seq_length - 1)]\n",
    "        \n",
    "        segment_ids = []\n",
    "        label_ids = []\n",
    "        label_mask.insert(0,1)\n",
    "        for i, token in enumerate(tokens):\n",
    "            segment_ids.append(0)\n",
    "            if len(labels) > i:\n",
    "                label_ids.append(label_map[labels[i]])\n",
    "        label_mask.append(1)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        label_mask = [1] * len(label_ids)\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            label_ids.append(0)\n",
    "            valid.append(0)\n",
    "            label_mask.append(0)\n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)\n",
    "            label_mask.append(0)\n",
    "        \n",
    "        print('input_ids', input_ids)\n",
    "        print('input_mask', input_mask)\n",
    "        print('segment_ids', segment_ids)\n",
    "        print('label_ids', label_ids)\n",
    "        print('valid', valid)\n",
    "        print('label_mask', label_mask)\n",
    "        print('\\n')\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "        assert len(valid) == max_seq_length\n",
    "        assert len(label_mask) == max_seq_length\n",
    "\n",
    "        if index < 3:\n",
    "            app.logger.info(\"*** Example ***\")\n",
    "            app.logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            app.logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            app.logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            app.logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            app.logger.info(\"valid: %s\" % \" \".join([str(x) for x in valid]))\n",
    "#             app.logger.info(\"label: %s (id = %d)\" % (labellist, label_ids))\n",
    "            app.logger.info(\"label_mask: %s\" % \" \".join([str(x) for x in label_mask]))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_ids,\n",
    "                              valid_ids=valid,\n",
    "                              label_mask=label_mask))\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "class IswPreprocessor:\n",
    "    def __init__(self, filename='data/test-full-isw-release.tsv'):\n",
    "        app.logger.info('------ Preprocssing ISW German corpus ------')\n",
    "        self.file = open(filename, encoding='utf-8')\n",
    "        self.sentences, self.labels, self.flat_labels = self.get_sentences_and_labels()\n",
    "        self.label_list = list(map(lambda x: x if x != 'NONE' else 'O', set(self.flat_labels)))\n",
    "        \n",
    "        app.logger.info(\"Number of sentences: {0} \".format(len(self.sentences)))\n",
    "        app.logger.info(\"Number of tags: {0} \".format(len(self.label_list)))\n",
    "\n",
    "    def get_sentences_and_labels(self):\n",
    "        \"\"\"\n",
    "        return : list of sentences : ['I have apple', 'I am here', 'hello ']\n",
    "        return : list of labels : ['O', 'O', 'B-GPE', ...]\n",
    "        \"\"\"\n",
    "        labels, label, sentences, sentence, flat_labels = [], [], [], [], []\n",
    "        for line in self.file:\n",
    "            if line.startswith(\"idx\") or line.startswith(\"0\") or line.startswith(\"NONE\"):\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            splits = line.split(\"\\t\")\n",
    "            if '?' in splits[2] or '.' in splits[2] :\n",
    "                if len(label)>0 and len(sentence)>0:\n",
    "                    sentences.append(\" \".join(sentence))\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "                continue\n",
    "            if splits[3] != 'NONE':\n",
    "                sentence.append(splits[3])\n",
    "                label.append(splits[6])\n",
    "                flat_labels.append(splits[6])\n",
    "\n",
    "        if len(label)>0 and len(sentence)>0:\n",
    "            sentences.append(\" \".join(sentence))\n",
    "            labels.append(label)\n",
    "\n",
    "        labels = [list(map(lambda x: x if x != 'NONE' else 'O', i)) for i in labels]\n",
    "\n",
    "        return sentences, labels, flat_labels\n",
    "\n",
    "    def get_tag2idx_idx2tag(self):\n",
    "        \"\"\"\n",
    "        return : dict of tag2idx : {'B-ADD': 0, 'B-AGE': 1, 'B-ART': 2, 'B-CARDINAL': 3,'B-CREAT': 4, ...}\n",
    "        return : dict of idx2tag : inverted\n",
    "        \"\"\"\n",
    "        tag2idx = {t: i for i, t in enumerate(sorted(self.label_list), 0)}\n",
    "        idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "        return tag2idx, idx2tag\n",
    "\n",
    "\n",
    "class TweetPreprocessor:\n",
    "    def __init__(self, filename='data/merged_headlines_annos.compact.tsv'):\n",
    "        app.logger.info('------ Preprocssing Tweets corpus ------')\n",
    "        self.file = open(filename, encoding='utf-8')\n",
    "        self.ners_vals=[]\n",
    "\n",
    "    def get_sentences_and_labels(self):\n",
    "        \"\"\"\n",
    "        return : list of sentences : ['I have apple', 'I am here', 'hello ']\n",
    "        return : list of labels : ['O', 'O', 'B-GPE', ...]\n",
    "        \"\"\"\n",
    "        labels, label, sentences, sentence, flat_labels = [], [], [], [], []\n",
    "        for line in self.file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip()\n",
    "            splits = line.split(\"\\t\")\n",
    "            if line.startswith(\"NONE\"):\n",
    "                if len(label)>0 and len(sentence)>0:\n",
    "                    sentences.append(\" \".join(sentence))\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "                continue\n",
    "            sentence.append(splits[1])\n",
    "            label.append(splits[3])\n",
    "            flat_labels.append(splits[3])\n",
    "        \n",
    "        if len(label)>0 and len(sentence)>0:\n",
    "            sentences.append(\" \".join(sentence))\n",
    "            labels.append(label)\n",
    "            \n",
    "        labels = [list(map(lambda x: x if x != 'NONE' else 'O', i)) for i in labels]\n",
    "        self.ners_vals = list(map(lambda x: x if x != 'NONE' else 'O', set(flat_labels)))\n",
    "\n",
    "        app.logger.info(\"Number of sentences: {0} \".format(len(sentences)))\n",
    "        app.logger.info(\"Number of tags: {0} \".format(len(self.ners_vals)))\n",
    "\n",
    "        return sentences, labels\n",
    "\n",
    "    def get_tag2idx_idx2tag(self):\n",
    "        \"\"\"\n",
    "        return : dict of tag2idx : {'B-ADD': 0, 'B-AGE': 1, 'B-ART': 2, 'B-CARDINAL': 3,'B-CREAT': 4, ...}\n",
    "        return : dict of idx2tag : inverted\n",
    "        \"\"\"\n",
    "        tag2idx = {t: i for i, t in enumerate(sorted(self.ners_vals), 1)}\n",
    "        idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "        return tag2idx, idx2tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For ISW data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences: 35\n",
      "num of tags : 18\n",
      "['nach Palästina nachher zu kommen', 'Also Sie haben uns ja jetzt schon sehr viel Interessantes erzählt und von Ihrer Jugendzeit in Wien könnten Sie uns da noch einmal ihre Eindrücke vermitteln']\n",
      "[['O', 'B-GPE', 'B-TIME', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'B-SORD', 'I-SORD', 'O', 'O', 'O']]\n",
      "['O', 'I-SORD', 'I-GPE', 'B-FREQ', 'B-AGE', 'B-NRP', 'B-DATE', 'I-ADD', 'B-SORD', 'B-TIME', 'B-TITLE', 'I-PER', 'B-ADD', 'B-GPE', 'B-PER', 'B-DUR', 'B-CARDINAL', 'B-FAC']\n"
     ]
    }
   ],
   "source": [
    "filename='data/test-full-isw-release.tsv'\n",
    "isw_pre = IswPreprocessor(filename)\n",
    "sentences = isw_pre.sentences\n",
    "labels = isw_pre.labels\n",
    "label_list = isw_pre.label_list\n",
    "\n",
    "\n",
    "\n",
    "print(\"number of sentences:\", len(sentences))\n",
    "print('num of tags :', len(label_list))\n",
    "i = 2\n",
    "print(sentences[:i])\n",
    "print(labels[:i])\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ADD': 0,\n",
       " 'B-AGE': 1,\n",
       " 'B-CARDINAL': 2,\n",
       " 'B-DATE': 3,\n",
       " 'B-DUR': 4,\n",
       " 'B-FAC': 5,\n",
       " 'B-FREQ': 6,\n",
       " 'B-GPE': 7,\n",
       " 'B-NRP': 8,\n",
       " 'B-PER': 9,\n",
       " 'B-SORD': 10,\n",
       " 'B-TIME': 11,\n",
       " 'B-TITLE': 12,\n",
       " 'I-ADD': 13,\n",
       " 'I-GPE': 14,\n",
       " 'I-PER': 15,\n",
       " 'I-SORD': 16,\n",
       " 'O': 17}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx, idx2tag = isw_pre.get_tag2idx_idx2tag()\n",
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  7, 11, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 11, 17, 17, 17, 17],\n",
       "       [17, 17, 17,  7, 17, 11, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17,  6, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 11, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 11, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 11, 17, 17, 17, 17, 17,  7],\n",
       "       [17, 17, 17, 17,  7, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [14,  5, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17,  8],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17,  8, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
       "       [17, 11, 17, 17, 11, 17, 17, 17, 17, 17]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=10, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_map {'O': 1, 'I-SORD': 2, 'I-GPE': 3, 'B-FREQ': 4, 'B-AGE': 5, 'B-NRP': 6, 'B-DATE': 7, 'I-ADD': 8, 'B-SORD': 9, 'B-TIME': 10, 'B-TITLE': 11, 'I-PER': 12, 'B-ADD': 13, 'B-GPE': 14, 'B-PER': 15, 'B-DUR': 16, 'B-CARDINAL': 17, 'B-FAC': 18}\n",
      "\n",
      "\n",
      "textlist ['nach', 'Palästina', 'nachher', 'zu', 'kommen']\n",
      "labellist ['O', 'B-GPE', 'B-TIME', 'O', 'O']\n",
      "input_ids [188, 24999, 188, 81, 1561, 0, 0, 0, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 14, 10, 1, 1, 0, 0, 0, 0, 0]\n",
      "valid [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Also', 'Sie', 'haben', 'uns', 'ja', 'jetzt', 'schon', 'sehr', 'viel', 'Interessantes', 'erzählt', 'und', 'von', 'Ihrer', 'Jugendzeit', 'in', 'Wien', 'könnten', 'Sie', 'uns', 'da', 'noch', 'einmal', 'ihre', 'Eindrücke', 'vermitteln']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DUR', 'O', 'B-GPE', 'O', 'O', 'O', 'O', 'B-SORD', 'I-SORD', 'O', 'O', 'O']\n",
      "input_ids [12482, 371, 474, 2099, 3278, 1868, 764, 1120, 870, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 10, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Meine', 'Eindrücke', 'von', 'Wien', 'von', 'damals']\n",
      "labellist ['O', 'O', 'O', 'B-GPE', 'O', 'B-TIME']\n",
      "input_ids [14066, 198, 88, 2319, 88, 2421, 0, 0, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 14, 1, 10, 0, 0, 0, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Also', 'ich', 'ich', 'glaub', 'ich', 'hatte', 'nur', 'gute', 'ehrlich', 'gestanden']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [12482, 1169, 1169, 7404, 1169, 466, 356, 4493, 6515, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Worauf', 'führ', 'ich', 'das', 'zurück']\n",
      "labellist ['O', 'O', 'O', 'O', 'O']\n",
      "input_ids [3147, 783, 1169, 93, 705, 0, 0, 0, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "valid [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Ich', 'kann', 'mich', 'erinnern', 'dass', 'man', 'mich', 'gefragt', 'hat', 'Na', 'was', 'willst', 'denn', 'werden']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1671, 479, 3277, 9508, 221, 478, 3277, 11558, 193, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Das', 'ist', 'doch', 'eine', 'sehr', 'aktuelle', 'Frage', 'immer', 'für', 'Kinder', 'und', 'ich', 'war', 'zwölf', 'und', 'ich', 'sehe', 'mich', 'bei', 'meiner', 'Tante', 'die', 'das', 'eben', 'auch', 'fragt', 'und', 'ich', 'sag', 'Jetzt', 'bin', 'ich', 'zwölf', 'eigentlich', 'würd', 'ich', 'ganz', 'gern', 'so', 'bleiben']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FREQ', 'O', 'O', 'O', 'O', 'O', 'B-AGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'B-AGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [295, 127, 1575, 155, 1120, 8974, 1685, 922, 142, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 4, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Und', 'das', 'ist', 'eigentlich', 'ich', 'wenn', 'so', 'ein', 'Kind', 'sagt', 'so', 'was', 'ohne', 'sich', 'darüber', 'den', 'Kopf', 'zu', 'zerbrechen', 'warum', 'sagt', 'sie', 'n', 'das', 'eigentlich']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1356, 93, 127, 4137, 1169, 557, 181, 39, 2383, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Aber', 'ich', 'kann', 'mich', 'dran', 'erinnern', 'also', 'muss', 'ich', 'doch', 'zufrieden', 'gewesen', 'sein', 'sonst', 'hätt', 'ich', 'das', 'nicht', 'gesagt']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1882, 1169, 479, 3277, 17880, 9508, 1638, 1080, 1169, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Und', 'ich', 'bin', 'ä', 'Jetzt', 'sag', 'ich', 'was', 'was', 'ich', 'nicht', 'sagen', 'sollte']\n",
      "labellist ['O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1356, 1169, 4058, 1062, 5072, 24925, 1169, 961, 961, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 10, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Denn', 'ich', 'bin', 'nur', 'in', 'die', 'Hauptschule', 'gegangen']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1566, 1169, 4058, 356, 50, 30, 910, 8807, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Es', 'hat', 'Leute', 'ja', 'schon', 'gegeben', 'das', 'brauchst', 'doch', 'niemand', 'sagen']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [482, 193, 5125, 3278, 764, 2108, 93, 6112, 1575, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Sag', 'ich', 'Aber', 'warum', 'soll', 'ich', 'das', 'nicht', 'sagen']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [773, 1169, 1882, 6024, 459, 1169, 93, 149, 4522, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Ich', 'hab', 'solche', 'Ich', 'ich', 'bin', 'viel', 'zu', 'ehrlich']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1671, 19009, 2264, 1671, 1169, 4058, 870, 81, 6515, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Ja', 'ich', 'bin', 'bestimmt', 'viel', 'zu', 'ehrlich']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [6802, 1169, 4058, 3828, 870, 81, 6515, 0, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Aber', 'ich', 'bin', 'eben', 'viel', 'zu', 'ehrlich', 'und', 'jetzt', 'bin', 'ich', 'alte', 'Frau', 'und', 'da', 'wer', 'ich', 'mich', 'nicht', 'mehr', 'ändern']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1882, 1169, 4058, 914, 870, 81, 6515, 42, 1868, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 10, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Und', 'ich', 'will', 'mich', 'vielleicht', 'auch', 'nicht', 'ändern']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1356, 1169, 1279, 3277, 5280, 194, 149, 7253, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Also', 'ich', 'bin', 'wirklich', 'nur', 'in', 'die', 'Hauptschule', 'gegangen', 'und', 'zwar', 'in', 'der', 'in', 'der', 'Schulgasse']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FAC']\n",
      "input_ids [12482, 1169, 4058, 4899, 356, 50, 30, 910, 8807, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Die', 'Schule', 'besteht', 'heute', 'noch', 'Sie', 'kennen', 'sich', 'in', 'Wien', 'nicht', 'aus']\n",
      "labellist ['O', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'O']\n",
      "input_ids [125, 3173, 1799, 1138, 357, 371, 5428, 144, 50, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 10, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Sie', 'kennen', 'sich', 'in', 'Wien', 'aus']\n",
      "labellist ['O', 'O', 'O', 'O', 'B-GPE', 'O']\n",
      "input_ids [371, 5428, 144, 50, 2319, 147, 0, 0, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 14, 1, 0, 0, 0, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Mh', 'ein', 'bisschen', 'also', 'Ja', 'ja', 'es', 'ist', 'egal', 'in', 'welchem', 'Bezirk', 'im', '18']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-GPE']\n",
      "input_ids [56, 39, 9564, 1638, 6802, 3278, 229, 127, 15601, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Bezirk', 'Schulgasse', 'ja', 'und', 'äh', 'die', 'Volksschule', 'war', 'übrigens', 'auch', 'in', 'der', 'Schulgasse', 'aber', 'dieses', 'Haus', 'steht', 'nicht', 'mehr']\n",
      "labellist ['I-GPE', 'B-FAC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-FAC', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [6052, 1238, 3278, 42, 1062, 30, 17614, 185, 15226, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [3, 18, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Aber', 'dies', 'die', 'Schule', 'v', 'in', 'der', 'Die', 'Hauptschule', 'in', 'der', 'Schulgasse', '59', 'die', 'steht', 'sehr', 'wohl', 'noch', 'und', 'wir', 'hatten', 'eine', 'sehr', 'gute', 'einen', 'sehr', 'guten', 'Klassenvorstand', 'und', 'zwar', 'die', 'Frau', 'Emma', 'Schrammek']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ADD', 'I-ADD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TITLE', 'B-PER', 'I-PER']\n",
      "input_ids [1882, 419, 30, 3173, 31, 50, 21, 125, 910, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Und', 'ich', 'glaub', 'sie', 'hat', 'äh', 'Sie', 'war', 'nicht', 'antisemitisch']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NRP']\n",
      "input_ids [1356, 1169, 7404, 213, 193, 1062, 371, 185, 149, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Wir', 'haben', 'es', 'jedenfalls', 'nicht', 'gemerkt', 'wir', 'waren', 'aber', 'nur', 'zwei', 'jüdische', 'Kinder', 'in', 'der', 'Klasse']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CARDINAL', 'B-NRP', 'O', 'O', 'O', 'O']\n",
      "input_ids [655, 474, 229, 3577, 149, 559, 232, 636, 386, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Und', 'das', 'ist', 'schon', 'irgendwie', 'etwas', 'Bedeutendes']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1356, 93, 127, 764, 17467, 1835, 2240, 0, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Die', 'Juden', 'haben', 'ihre', 'Kinder', 'nicht', 'in', 'die', 'Volks', 'in', 'die', 'Hauptschule', 'geschickt', 'sondern', 'ins', 'Gymnasium']\n",
      "labellist ['O', 'B-NRP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [125, 5953, 474, 682, 1068, 149, 50, 30, 2238, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 6, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Alle']\n",
      "labellist ['O']\n",
      "input_ids [3939, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "input_mask [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "valid [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_mask [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Alle', 'meine', 'Freundinnen', 'die', 'ich', 'außerhalb', 'der', 'Schule', 'hatte', 'die', 'waren', 'im', 'Gymnasium']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [3939, 6667, 9742, 30, 1169, 4795, 21, 3173, 466, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Und', 'warum', 'man', 'mich', 'nicht', 'ins', 'Gymnasium', 'geschickt', 'hat', 'hat', 'verschiedene', 'Gründe']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1356, 6024, 478, 3277, 149, 539, 7730, 11165, 193, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Ob', 'ich', 'die', 'Aufnahmsprüfung', 'nicht', 'bestanden', 'hätte', 'das', 'weiß', 'ich', 'nicht', 'das', 'kann', 'ich', 'heut', 'nicht', 'sagen']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O']\n",
      "input_ids [1448, 1169, 30, 24985, 149, 6929, 1807, 93, 3275, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Ich', 'hab', 'nicht', 'besonders', 'gern', 'gelernt', 'das', 'weiß', 'ich']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1671, 19009, 149, 2406, 8942, 15652, 93, 3275, 1169, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Aber', 'meine', 'Eltern', 'hatten', 'auch', 'nicht', 'die', 'Mitteln', 'dazu']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1882, 6667, 2579, 1520, 194, 149, 30, 11544, 1425, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n",
      "textlist ['Das', 'hat', 'ja', 'viel', 'Geld', 'gekostet']\n",
      "labellist ['O', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [295, 193, 3278, 870, 1761, 2858, 0, 0, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "textlist ['Und', 'dann', 'hat', 'man', 'dann', 'eben', 'in', 'der', 'Familie', 'gesagt']\n",
      "labellist ['O', 'B-TIME', 'O', 'O', 'B-TIME', 'O', 'O', 'O', 'O', 'O']\n",
      "input_ids [1356, 670, 193, 478, 670, 914, 50, 21, 1786, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_ids [1, 10, 1, 1, 10, 1, 1, 1, 1, 0]\n",
      "valid [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\", do_lower_case=False)\n",
    "\n",
    "features = convert_examples_to_features(all_sentences=sentences, all_labels=labels, label_list=label_list, max_seq_length=10, tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'I-SORD': 2,\n",
       " 'I-GPE': 3,\n",
       " 'B-FREQ': 4,\n",
       " 'B-AGE': 5,\n",
       " 'B-NRP': 6,\n",
       " 'B-DATE': 7,\n",
       " 'I-ADD': 8,\n",
       " 'B-SORD': 9,\n",
       " 'B-TIME': 10,\n",
       " 'B-TITLE': 11,\n",
       " 'I-PER': 12,\n",
       " 'B-ADD': 13,\n",
       " 'B-GPE': 14,\n",
       " 'B-PER': 15,\n",
       " 'B-DUR': 16,\n",
       " 'B-CARDINAL': 17,\n",
       " 'B-FAC': 18}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = {label : i for i, label in enumerate(label_list,1)}\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.InputFeatures object at 0x14099c828>\n",
      "input_ids [188, 24999, 188, 81, 1561, 0, 0, 0, 0, 0]\n",
      "input_mask [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label_id [1, 14, 10, 1, 1, 0, 0, 0, 0, 0]\n",
      "valid_ids [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "label_mask [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "label_map =  {'O': 1, 'I-SORD': 2, 'I-GPE': 3, 'B-FREQ': 4, 'B-AGE': 5, 'B-NRP': 6, 'B-DATE': 7, 'I-ADD': 8, 'B-SORD': 9, 'B-TIME': 10, 'B-TITLE': 11, 'I-PER': 12, 'B-ADD': 13, 'B-GPE': 14, 'B-PER': 15, 'B-DUR': 16, 'B-CARDINAL': 17, 'B-FAC': 18}\n",
    "textlist = ['nach', 'Palästina', 'nachher', 'zu', 'kommen']\n",
    "labellist = ['O', 'B-GPE', 'B-TIME', 'O', 'O']\n",
    "print(features[i])\n",
    "print('input_ids',features[i].input_ids)\n",
    "print('input_mask',features[i].input_mask)\n",
    "print('segment_ids',features[i].segment_ids)\n",
    "print('label_id',features[i].label_id)\n",
    "print('valid_ids',features[i].valid_ids)\n",
    "print('label_mask',features[i].label_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list.append(\"[CLS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list.append(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'I-SORD',\n",
       " 'I-GPE',\n",
       " 'B-FREQ',\n",
       " 'B-AGE',\n",
       " 'B-NRP',\n",
       " 'B-DATE',\n",
       " 'I-ADD',\n",
       " 'B-SORD',\n",
       " 'B-TIME',\n",
       " 'B-TITLE',\n",
       " 'I-PER',\n",
       " 'B-ADD',\n",
       " 'B-GPE',\n",
       " 'B-PER',\n",
       " 'B-DUR',\n",
       " 'B-CARDINAL',\n",
       " 'B-FAC',\n",
       " '[CLS]',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Preprocssing Tweets corpus ------\n",
      "Total number of tweets 8957\n",
      "Total number of ner tags in tweets 63\n",
      "number of sentences: 8957\n",
      "num of tags : 63\n",
      "Wann auch immer der #Brexit kommen mag - die #IHK bereitet die Unternehmen in #Rheinhessen darauf vor und empfiehlt , vom Worst-Case-Szenario auszugehen . [ plus-Inhalt ]\n",
      "['O', 'O', 'O', 'O', 'B-EVT', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "{'B-AGE': 0, 'B-ART': 1, 'B-CARDINAL': 2, 'B-CREAT': 3, 'B-DATE': 4, 'B-DUR': 5, 'B-EVT': 6, 'B-FAC': 7, 'B-FRAC': 8, 'B-FREQ': 9, 'B-GPE': 10, 'B-LAN': 11, 'B-LAW': 12, 'B-LOC': 13, 'B-MED': 14, 'B-MISC': 15, 'B-MON': 16, 'B-NRP': 17, 'B-ORDINAL': 18, 'B-ORG': 19, 'B-PER': 20, 'B-PERC': 21, 'B-PRODUCT': 22, 'B-PROJ': 23, 'B-QUANT': 24, 'B-RATE': 25, 'B-SCORE': 26, 'B-SORD': 27, 'B-TIME': 28, 'B-TITLE': 29, 'B-URL': 30, 'I-AGE': 31, 'I-ART': 32, 'I-CARDINAL': 33, 'I-CREAT': 34, 'I-DATE': 35, 'I-DUR': 36, 'I-EVT': 37, 'I-FAC': 38, 'I-FRAC': 39, 'I-FREQ': 40, 'I-GPE': 41, 'I-LAN': 42, 'I-LAW': 43, 'I-LOC': 44, 'I-MED': 45, 'I-MISC': 46, 'I-MON': 47, 'I-NRP': 48, 'I-ORDINAL': 49, 'I-ORG': 50, 'I-PER': 51, 'I-PERC': 52, 'I-PRODUCT': 53, 'I-PROJ': 54, 'I-QUANT': 55, 'I-RATE': 56, 'I-SCORE': 57, 'I-SORD': 58, 'I-TIME': 59, 'I-TITLE': 60, 'I-URL': 61, 'O': 62}\n",
      "{0: 'B-AGE', 1: 'B-ART', 2: 'B-CARDINAL', 3: 'B-CREAT', 4: 'B-DATE', 5: 'B-DUR', 6: 'B-EVT', 7: 'B-FAC', 8: 'B-FRAC', 9: 'B-FREQ', 10: 'B-GPE', 11: 'B-LAN', 12: 'B-LAW', 13: 'B-LOC', 14: 'B-MED', 15: 'B-MISC', 16: 'B-MON', 17: 'B-NRP', 18: 'B-ORDINAL', 19: 'B-ORG', 20: 'B-PER', 21: 'B-PERC', 22: 'B-PRODUCT', 23: 'B-PROJ', 24: 'B-QUANT', 25: 'B-RATE', 26: 'B-SCORE', 27: 'B-SORD', 28: 'B-TIME', 29: 'B-TITLE', 30: 'B-URL', 31: 'I-AGE', 32: 'I-ART', 33: 'I-CARDINAL', 34: 'I-CREAT', 35: 'I-DATE', 36: 'I-DUR', 37: 'I-EVT', 38: 'I-FAC', 39: 'I-FRAC', 40: 'I-FREQ', 41: 'I-GPE', 42: 'I-LAN', 43: 'I-LAW', 44: 'I-LOC', 45: 'I-MED', 46: 'I-MISC', 47: 'I-MON', 48: 'I-NRP', 49: 'I-ORDINAL', 50: 'I-ORG', 51: 'I-PER', 52: 'I-PERC', 53: 'I-PRODUCT', 54: 'I-PROJ', 55: 'I-QUANT', 56: 'I-RATE', 57: 'I-SCORE', 58: 'I-SORD', 59: 'I-TIME', 60: 'I-TITLE', 61: 'I-URL', 62: 'O'}\n",
      "['I-SORD', 'B-NRP', 'B-FREQ', 'B-DATE', 'I-SCORE', 'I-CREAT', 'B-TITLE', 'B-AGE', 'I-LAW', 'B-LOC', 'B-URL', 'B-PER', 'B-MON', 'B-PRODUCT', 'B-GPE', 'B-ART', 'B-MISC', 'I-MISC', 'I-ORG', 'I-FRAC', 'B-SCORE', 'B-SORD', 'I-CARDINAL', 'I-PER', 'I-DUR', 'I-NRP', 'I-ART', 'I-EVT', 'B-CARDINAL', 'I-PROJ', 'B-ORDINAL', 'O', 'I-GPE', 'B-QUANT', 'I-MON', 'B-MED', 'B-LAW', 'I-PERC', 'B-EVT', 'B-RATE', 'B-FRAC', 'B-PROJ', 'I-AGE', 'I-LAN', 'I-TITLE', 'I-FAC', 'B-LAN', 'B-TIME', 'I-MED', 'I-ORDINAL', 'B-FAC', 'B-ORG', 'I-QUANT', 'I-DATE', 'I-FREQ', 'B-DUR', 'I-PRODUCT', 'I-LOC', 'I-URL', 'I-RATE', 'B-PERC', 'I-TIME', 'B-CREAT']\n"
     ]
    }
   ],
   "source": [
    "filename='data/merged_headlines_annos.compact.tsv'\n",
    "\n",
    "tweet_pre = TweetPreprocessor(filename)\n",
    "sentences, labels = tweet_pre.get_list_of_sentences_labels()\n",
    "tag2idx, idx2tag = tweet_pre.get_tag2idx_idx2tag()\n",
    "\n",
    "print(\"number of sentences:\", len(sentences))\n",
    "print('num of tags :', len(tweet_pre.ners_vals))\n",
    "i = 2\n",
    "print(sentences[i])\n",
    "print(labels[i])\n",
    "print(tag2idx)\n",
    "print(idx2tag)\n",
    "print(tweet_pre.ners_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-SCORE', 'B-URL', 'I-CREAT', 'I-SCORE', 'I-URL'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tweet_pre.ners_vals)-set(isw_pre.ners_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-ADD', 'I-ADD'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(isw_pre.ners_vals)-set(tweet_pre.ners_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'Also', 'Sie', 'haben', 'uns', 'ja', 'jetzt'],\n",
       " ['Die', 'zwei'],\n",
       " ['Jetzt'],\n",
       " ['Ja', 'dann'],\n",
       " ['nach'],\n",
       " ['Nach'],\n",
       " ['Jetzt', 'Jetzt']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\", do_lower_case=False)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "tokenized_texts = [[ll for ll in e if \"##\" not in ll] for e in tokenized_texts ]\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'O', 'O', 'O', 'O', 'O', 'B-TIME'],\n",
       " ['O', 'B-CARDINAL'],\n",
       " ['B-TIME'],\n",
       " ['O', 'B-TIME'],\n",
       " ['B-TIME'],\n",
       " ['B-TIME'],\n",
       " ['B-TIME', 'B-TIME']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  103, 26938, 12482,   371,   474,  2099,  3278,  1868],\n",
       "       [  125,   382,     0,     0,     0,     0,     0,     0],\n",
       "       [ 5072,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [ 6802,   670,     0,     0,     0,     0,     0,     0],\n",
       "       [  188,   320,     0,     0,     0,     0,     0,     0],\n",
       "       [  326,   320,     0,     0,     0,     0,     0,     0],\n",
       "       [ 5072,  5072,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len=9\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "#                                         maxlen=max_len, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 2, 2, 2, 1],\n",
       "       [2, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "#                                         maxlen=max_len,  \n",
    "                                         padding=\"post\",\n",
    "                                        dtype=\"long\", truncating=\"post\")\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
    "attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import json\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from preprocessor.preprocessor import *\n",
    "from transformers import BertTokenizer, BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ner:\n",
    "    def __init__(self,model_dir: str):\n",
    "        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n",
    "        self.label_map = self.model_config[\"label_map\"]\n",
    "        self.max_seq_length = self.model_config[\"max_seq_length\"]\n",
    "        self.max_seq_length = 10\n",
    "        self.label_map = {int(k):v for k,v in self.label_map.items()}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self, model_dir: str, model_config: str = \"model_config.json\"):\n",
    "        model_config = os.path.join(model_dir,model_config)\n",
    "        model_config = json.load(open(model_config))\n",
    "#         model = BertForTokenClassification.from_pretrained(model_dir)\n",
    "#         tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "        model = BertForTokenClassification.from_pretrained('bert-base-uncased')\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        return model, tokenizer, model_config\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\" tokenize input\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        tokens = []\n",
    "        valid_positions = []\n",
    "        for i,word in enumerate(words):\n",
    "            token = self.tokenizer.tokenize(word)\n",
    "            token = [ t for t in token if \"##\" not in t ]\n",
    "            tokens.extend(token)\n",
    "            for i in range(len(token)):\n",
    "                if i == 0:\n",
    "                    valid_positions.append(1)\n",
    "                else:\n",
    "                    valid_positions.append(0)\n",
    "        return tokens, valid_positions\n",
    "\n",
    "    def preprocess(self, text: str):\n",
    "        \"\"\" preprocess \"\"\"\n",
    "        tokens, valid_positions = self.tokenize(text)\n",
    "        segment_ids = []\n",
    "        for i in range(len(tokens)):\n",
    "            segment_ids.append(0)\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        while len(input_ids) < self.max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            valid_positions.append(0)\n",
    "        return input_ids,input_mask,segment_ids,valid_positions\n",
    "\n",
    "    def predict(self, text: str):\n",
    "        input_ids,input_mask,segment_ids,valid_ids = self.preprocess(text)\n",
    "        input_ids = torch.tensor([input_ids],dtype=torch.long,device=self.device)\n",
    "        input_mask = torch.tensor([input_mask],dtype=torch.long,device=self.device)\n",
    "        segment_ids = torch.tensor([segment_ids],dtype=torch.long,device=self.device)\n",
    "        valid_ids = torch.tensor([valid_ids],dtype=torch.long,device=self.device)\n",
    "#         print('input_ids', input_ids)\n",
    "#         print('input_mask', input_mask)\n",
    "#         print('segment_ids', segment_ids)\n",
    "#         print('valid_ids', valid_ids)\n",
    "#         print('valid_ids[0]', valid_ids[0])\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, segment_ids, input_mask,valid_ids)\n",
    "            logits = outputs[0]\n",
    "#             print('logit type', type(logits))\n",
    "#             print('logit ', logits)\n",
    "        logits = F.softmax(logits,dim=2)\n",
    "        logits_label = torch.argmax(logits,dim=2)\n",
    "        logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n",
    "#         print('logits_label:', logits_label)\n",
    "        logits_confidence = [values[label].item() for values,label in zip(logits[0],logits_label)]\n",
    "#         print('logits_confidence', logits_confidence)\n",
    "\n",
    "        logits = []\n",
    "        for index,mask in enumerate(valid_ids[0]):\n",
    "            if mask == 1:\n",
    "#                 print('hi', mask)\n",
    "                logits.append((logits_label[index], logits_confidence[index]))\n",
    "#                 print('app logits', logits)\n",
    "            else:\n",
    "                pass\n",
    "        print('label_map', self.label_map)\n",
    "#         print('logit.pop', logits)\n",
    "        labels = [(self.label_map[label],confidence) for label,confidence in logits]\n",
    "        words = word_tokenize(text)\n",
    "#         print('words:', words)\n",
    "#         print('labels:', labels)\n",
    "        assert len(labels) == len(words)\n",
    "        output = [{\"word\":word,\"tag\":label,\"confidence\":confidence} for word,(label,confidence) in zip(words,labels)]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences ='jetzt bin ich zwölf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jetzt bin ich zwölf'"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6031a07a42334265916605ed5e000f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-424-6288ee0b3d15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/steve.chen/thesis/thesis-ner-co-tri-training/models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-421-81318b19cb62>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_seq_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-421-81318b19cb62>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_dir, model_config)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         model = BertForTokenClassification.from_pretrained(model_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         tokenizer = BertTokenizer.from_pretrained(model_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0mforce_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                     \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m                 )\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         )\n\u001b[1;32m    240\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0ms3_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storing %s in cache at %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, user_agent)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetEffectiveLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     )\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m                 if (\n\u001b[1;32m    509\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ner/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ner = Ner(\"/Users/steve.chen/thesis/thesis-ner-co-tri-training/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jetzt', 'bin', 'ich', 'zwölf']\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokens, valid_positions = ner.tokenize(sentences)\n",
    "print(tokens)\n",
    "print(valid_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids,input_mask,segment_ids,valid_positions = ner.preprocess(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1868, 4058, 1169, 4420, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)\n",
    "print(input_mask)\n",
    "print(segment_ids)\n",
    "print(valid_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_map {0: 'B-ADD', 1: 'B-AGE', 2: 'B-CARDINAL', 3: 'B-DATE', 4: 'B-DUR', 5: 'B-FAC', 6: 'B-FREQ', 7: 'B-GPE', 8: 'B-NRP', 9: 'B-PER', 10: 'B-SORD', 11: 'B-TIME', 12: 'B-TITLE', 13: 'I-ADD', 14: 'I-GPE', 15: 'I-PER', 16: 'I-SORD', 17: 'O'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'jetzt', 'tag': 'B-ADD', 'confidence': 0.6784576177597046},\n",
       " {'word': 'bin', 'tag': 'B-ADD', 'confidence': 0.5905336737632751},\n",
       " {'word': 'ich', 'tag': 'B-ADD', 'confidence': 0.7070490717887878},\n",
       " {'word': 'zwölf', 'tag': 'B-ADD', 'confidence': 0.6636413931846619}]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[1169, 1631,  188,    0,    0,    0,    0,    0,    0,    0]])\n",
      "input_mask tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
      "segment_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "valid_ids tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])\n",
      "valid_ids[0] tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n",
      "logit type <class 'torch.Tensor'>\n",
      "logit  tensor([[[-0.1963,  0.8812, -0.6778],\n",
      "         [-0.1679,  0.6354, -0.4804],\n",
      "         [-0.4352,  0.9377, -0.6370],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636],\n",
      "         [-0.1695,  1.2770, -0.6636]]])\n",
      "logits_label: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "logits_confidence [0.6448203325271606, 0.5632150173187256, 0.684721052646637, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963, 0.7251530289649963]\n",
      "0 tensor(1)\n",
      "hi tensor(1)\n",
      "app logits [(1, 0.6448203325271606)]\n",
      "1 tensor(1)\n",
      "hi tensor(1)\n",
      "app logits [(1, 0.6448203325271606), (1, 0.5632150173187256)]\n",
      "2 tensor(1)\n",
      "hi tensor(1)\n",
      "app logits [(1, 0.6448203325271606), (1, 0.5632150173187256), (1, 0.684721052646637)]\n",
      "3 tensor(0)\n",
      "4 tensor(0)\n",
      "5 tensor(0)\n",
      "6 tensor(0)\n",
      "7 tensor(0)\n",
      "8 tensor(0)\n",
      "9 tensor(0)\n",
      "label_map {0: 'B-CARDINAL', 1: 'B-TIME', 2: 'O'}\n",
      "logit.pop [(1, 0.6448203325271606), (1, 0.5632150173187256), (1, 0.684721052646637)]\n",
      "words: ['ich', 'werde', 'nacher']\n",
      "labels: [('B-TIME', 0.6448203325271606), ('B-TIME', 0.5632150173187256), ('B-TIME', 0.684721052646637)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'ich', 'tag': 'B-TIME', 'confidence': 0.6448203325271606},\n",
       " {'word': 'werde', 'tag': 'B-TIME', 'confidence': 0.5632150173187256},\n",
       " {'word': 'nacher', 'tag': 'B-TIME', 'confidence': 0.684721052646637}]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences ='jetzt bin ich zwölf'\n",
    "ner.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Ner:\n",
    "    def __init__(self,model_dir: str):\n",
    "        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n",
    "        self.label_map = self.model_config[\"label_map\"]\n",
    "        self.max_seq_length = self.model_config[\"max_seq_length\"]\n",
    "        self.label_map = {int(k):v for k,v in self.label_map.items()}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self, model_dir: str, model_config: str = \"model_config.json\"):\n",
    "        model_config = os.path.join(model_dir,model_config)\n",
    "        model_config = json.load(open(model_config))\n",
    "        model = BertForTokenClassification.from_pretrained(model_dir)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "        return model, tokenizer, model_config\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        \"\"\" tokenize input\"\"\"\n",
    "        words = word_tokenize(text)\n",
    "        tokens = []\n",
    "        valid_positions = []\n",
    "        for i,word in enumerate(words):\n",
    "            token = self.tokenizer.tokenize(word)\n",
    "            token = [ t for t in token if \"##\" not in t ]\n",
    "            tokens.extend(token)\n",
    "            for i in range(len(token)):\n",
    "                if i == 0:\n",
    "                    valid_positions.append(1)\n",
    "                else:\n",
    "                    valid_positions.append(0)\n",
    "        return tokens, valid_positions\n",
    "\n",
    "    def preprocess(self, text: str):\n",
    "        \"\"\" preprocess \"\"\"\n",
    "        tokens, valid_positions = self.tokenize(text)\n",
    "        segment_ids = []\n",
    "        for i in range(len(tokens)):\n",
    "            segment_ids.append(0)\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        while len(input_ids) < self.max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "            valid_positions.append(0)\n",
    "        return input_ids,input_mask,segment_ids,valid_positions\n",
    "\n",
    "    def predict(self, text: str):\n",
    "        input_ids,input_mask,segment_ids,valid_ids = self.preprocess(text)\n",
    "        input_ids = torch.tensor([input_ids],dtype=torch.long,device=self.device)\n",
    "        input_mask = torch.tensor([input_mask],dtype=torch.long,device=self.device)\n",
    "        segment_ids = torch.tensor([segment_ids],dtype=torch.long,device=self.device)\n",
    "        valid_ids = torch.tensor([valid_ids],dtype=torch.long,device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids, segment_ids, input_mask,valid_ids)\n",
    "            logits = outputs[0]\n",
    "\n",
    "        logits = F.softmax(logits,dim=2)\n",
    "        logits_label = torch.argmax(logits,dim=2)\n",
    "        logits_label = logits_label.detach().cpu().numpy().tolist()[0]\n",
    "        print('logits_label: ', logits_label)\n",
    "        logits_confidence = [values[label].item() for values,label in zip(logits[0],logits_label)]\n",
    "        print('logits_confidence: ', logits_confidence)\n",
    "        print('valid_ids[0]: ', valid_ids[0])\n",
    "        logits = []\n",
    "        for index,mask in enumerate(valid_ids[0]):\n",
    "            if mask == 1:\n",
    "                logits.append((logits_label[index], logits_confidence[index]))\n",
    "            else:\n",
    "                pass\n",
    "        print('logits: ', logits)\n",
    "        print('self.label_map: ', self.label_map)\n",
    "        labels = [(self.label_map[label],confidence) for label,confidence in logits]\n",
    "        print('labels: ', labels)\n",
    "        words = word_tokenize(text)\n",
    "        assert len(labels) == len(words)\n",
    "        pre_tags = [self.label_map[label] for label, _ in logits]\n",
    "        print('!!!! pre_tags: ', pre_tags)\n",
    "        output = [{\"word:\":word,\"tag\":label,\"confidence\":confidence} for word,(label,confidence) in zip(words,labels)]\n",
    "        return output\n",
    "\n",
    "\n",
    "class Preditor:\n",
    "    \"\"\"\n",
    "    This class will make predictions for unlabeled data:\n",
    "    :input : list of sentences\n",
    "    :return : list of predicted tags\n",
    "    \"\"\"\n",
    "    def __init__(self,model_dir: str):\n",
    "        self.model , self.tokenizer, self.model_config = self.load_model(model_dir)\n",
    "        self.label_map = self.model_config[\"label_map\"]\n",
    "        self.max_seq_length = self.model_config[\"max_seq_length\"]\n",
    "        self.label_map = {int(k):v for k,v in self.label_map.items()}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self, model_dir: str, model_config: str = \"model_config.json\"):\n",
    "        model_config = os.path.join(model_dir,model_config)\n",
    "        model_config = json.load(open(model_config))\n",
    "        model = BertForTokenClassification.from_pretrained(model_dir)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "        return model, tokenizer, model_config\n",
    "    \n",
    "    def tokenize(self, sentences:list):\n",
    "        \"\"\"\n",
    "        :input: list of sentences\n",
    "        :return: list of tokens, list of valid_positions\n",
    "        \"\"\"\n",
    "        all_tokens = []\n",
    "        all_valid_positions = []\n",
    "        for text in sentences:\n",
    "            words = word_tokenize(text)\n",
    "            tokens = []\n",
    "            valid_positions = []\n",
    "            for i, word in enumerate(words):\n",
    "                token = self.tokenizer.tokenize(word)\n",
    "                token = [t for t in token if \"##\" not in t]\n",
    "                tokens.extend(token)\n",
    "                for i in range(len(token)):\n",
    "                    if i == 0:\n",
    "                        valid_positions.append(1)\n",
    "                    else:\n",
    "                        valid_positions.append(0)\n",
    "            all_tokens.append(tokens)\n",
    "            all_valid_positions.append(valid_positions)\n",
    "        \n",
    "        return all_tokens, all_valid_positions\n",
    "\n",
    "    def preprocess(self, sentences:list):\n",
    "        all_tokens, all_valid_positions = self.tokenize(sentences)\n",
    "\n",
    "        all_input_ids = []\n",
    "        all_input_mask = []\n",
    "        all_segment_ids = []\n",
    "        all_pad_valid_positions = []\n",
    "        for index, tokens in enumerate(all_tokens):\n",
    "            segment_ids = []\n",
    "            for i in range(len(tokens)):\n",
    "                segment_ids.append(0)\n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            input_mask = [1] * len(input_ids)\n",
    "            while len(input_ids) < self.max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "                all_valid_positions[index].append(0)\n",
    "            all_input_ids.append(input_ids)\n",
    "            all_input_mask.append(input_mask)\n",
    "            all_segment_ids.append(segment_ids)\n",
    "            all_pad_valid_positions.append(all_valid_positions[index])\n",
    "        \n",
    "        return all_input_ids, all_input_mask, all_segment_ids, all_pad_valid_positions\n",
    "    \n",
    "    def predict(self, sentences:list):\n",
    "        all_input_ids, all_input_mask, all_segment_ids, all_pad_valid_positions = self.preprocess(sentences)\n",
    "        all_input_ids=torch.tensor(all_input_ids)\n",
    "        all_input_mask=torch.tensor(all_input_mask)\n",
    "        all_segment_ids=torch.tensor(all_segment_ids)\n",
    "        all_pad_valid_positions=torch.tensor(all_pad_valid_positions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(all_input_ids, all_segment_ids, all_input_mask,all_pad_valid_positions)\n",
    "            logits = outputs[0]\n",
    "#             print('outputs', outputs)\n",
    "#             print('outputs[0]', logits)\n",
    "\n",
    "        logits = F.softmax(logits,dim=2)\n",
    "        logits_label = [list(p) for p in torch.argmax(logits, axis=2)]\n",
    "        print('logits_label before: ', logits_label)\n",
    "        logits_label = [logits_label[i].detach().cpu().numpy().tolist()[0] for i in range(len(logits_label))]\n",
    "        print('logits_label: ', logits_label)\n",
    "        logits_confidence = [values[label].item() for values,label in zip(logits[0],logits_label)]\n",
    "        print('logits_confidence: ', logits_confidence)\n",
    "        print('valid_ids[0]: ', valid_ids[0])\n",
    "        logits = []\n",
    "        for index,mask in enumerate(valid_ids[0]):\n",
    "            if mask == 1:\n",
    "                logits.append((logits_label[index], logits_confidence[index]))\n",
    "            else:\n",
    "                pass\n",
    "        print('logits: ', logits)\n",
    "        print('self.label_map: ', self.label_map)\n",
    "        labels = [(self.label_map[label],confidence) for label,confidence in logits]\n",
    "        print('labels: ', labels)\n",
    "        words = word_tokenize(text)\n",
    "        assert len(labels) == len(words)\n",
    "        pre_tags = [self.label_map[label] for label, _ in logits]\n",
    "        print('!!!! pre_tags: ', pre_tags)\n",
    "        output = [{\"word:\":word,\"tag\":label,\"confidence\":confidence} for word,(label,confidence) in zip(words,labels)]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Also', 'Sie', 'haben', 'uns', 'ja', 'jetzt']\n",
      "[1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"./models\"\n",
    "model = Ner(model_dir)\n",
    "sentences = 'Also Sie haben uns ja jetzt'\n",
    "all_tokens, all_valid_positions = model.tokenize(sentences)\n",
    "print(all_tokens)\n",
    "print(all_valid_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12482, 371, 474, 2099, 3278, 1868]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = model.tokenizer.convert_tokens_to_ids(all_tokens)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_input_ids [12482, 371, 474, 2099, 3278, 1868, 0, 0, 0, 0]\n",
      "\n",
      " all_input_mask [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "\n",
      " all_segment_ids [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " all_valid_positions [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "all_input_ids, all_input_mask, all_segment_ids, all_valid_positions = model.preprocess(sentences)\n",
    "print('all_input_ids', all_input_ids)\n",
    "print('\\n all_input_mask', all_input_mask)\n",
    "print('\\n all_segment_ids', all_segment_ids)\n",
    "print('\\n all_valid_positions', all_valid_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_input_ids [[12482, 371, 474, 2099, 3278, 1868, 764, 1120, 870, 13769], [14066, 198, 88, 2319, 88, 2421, 0, 0, 0, 0], [12482, 1169, 7404, 1169, 466, 356, 4493, 6515, 12059, 0], [3147, 783, 1169, 93, 705, 0, 0, 0, 0, 0]]\n",
      "\n",
      " all_input_mask [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]\n",
      "\n",
      " all_segment_ids [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      " all_valid_positions [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "all_input_ids, all_input_mask, all_segment_ids, all_valid_positions = model.preprocess(sentences)\n",
    "print('all_input_ids', all_input_ids)\n",
    "print('\\n all_input_mask', all_input_mask)\n",
    "print('\\n all_segment_ids', all_segment_ids)\n",
    "print('\\n all_valid_positions', all_valid_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_label:  [[tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17)], [tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17)], [tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17)], [tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17), tensor(17)]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-83d0a5722f9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-8098c3719c97>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mlogits_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logits_label: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mlogits_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlogits_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logits_label: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mlogits_confidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-8098c3719c97>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mlogits_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logits_label: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mlogits_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlogits_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logits_label: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mlogits_confidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "output = model.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_label before detach tensor([[17, 17, 17, 17, 17, 17, 17, 17, 17, 17]])\n",
    "logits_label:  [17, 17, 17, 17, 17, 17, 17, 17, 17, 17]\n",
    "logits_confidence:  [0.9269393086433411, 0.9271261692047119, 0.9179024696350098, 0.9214142560958862, 0.9159783124923706, 0.8886045217514038, 0.8886045217514038, 0.8886045217514038, 0.8886046409606934, 0.8886046409606934]\n",
    "valid_ids[0]:  tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "logits:  [(17, 0.9269393086433411), (17, 0.9271261692047119), (17, 0.9179024696350098), (17, 0.9214142560958862), (17, 0.9159783124923706)]\n",
    "self.label_map:  {0: 'B-ADD', 1: 'B-AGE', 2: 'B-CARDINAL', 3: 'B-DATE', 4: 'B-DUR', 5: 'B-FAC', 6: 'B-FREQ', 7: 'B-GPE', 8: 'B-NRP', 9: 'B-PER', 10: 'B-SORD', 11: 'B-TIME', 12: 'B-TITLE', 13: 'I-ADD', 14: 'I-GPE', 15: 'I-PER', 16: 'I-SORD', 17: 'O'}\n",
    "labels:  [('O', 0.9269393086433411), ('O', 0.9271261692047119), ('O', 0.9179024696350098), ('O', 0.9214142560958862), ('O', 0.9159783124923706)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [210, 1, 2, 20,1 ,20 ,210, 6, 210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 6, 210, 20]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-german-cased')\n",
    "\n",
    "input_ids = torch.tensor(tokenizer.encode(\"ich bin jetzet zehn\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids, labels=labels)\n",
    "\n",
    "loss, scores = outputs[:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    3,  1169,  4058, 11991,    75,  1969,     4]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4811, grad_fn=<NllLossBackward>),\n",
       " tensor([[[-0.2903,  0.1528],\n",
       "          [-0.2493,  0.2733],\n",
       "          [-0.0438,  0.0960],\n",
       "          [ 0.1222,  0.6178],\n",
       "          [-0.3988, -0.0919],\n",
       "          [-0.2724,  0.2836],\n",
       "          [-0.7181,  0.3331]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4811, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2903,  0.1528],\n",
       "         [-0.2493,  0.2733],\n",
       "         [-0.0438,  0.0960],\n",
       "         [ 0.1222,  0.6178],\n",
       "         [-0.3988, -0.0919],\n",
       "         [-0.2724,  0.2836],\n",
       "         [-0.7181,  0.3331]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
